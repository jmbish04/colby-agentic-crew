{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6cbbQjG71eh"
      },
      "source": [
        "# Colby Agentic Crew - GenAI + GitHub Ops\n",
        "\n",
        "This notebook contains the complete agent script. Run the cells in order.\n",
        "\n",
        "**First, run this cell to install the required Python libraries.**"
      ],
      "id": "P6cbbQjG71eh"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdtL9Fqr71ei",
        "outputId": "5cb7b51d-f9ba-4de5-9051-42f522442b22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m432.7/432.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# In a Colab cell, run this first to install dependencies:\n",
        "!pip -q install google-genai PyGithub requests tiktoken\n"
      ],
      "id": "fdtL9Fqr71ei"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yV-MroW71ei"
      },
      "source": [
        "## 1. Imports"
      ],
      "id": "2yV-MroW71ei"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PE6PXwdp71ei"
      },
      "outputs": [],
      "source": [
        "# @title  imports\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import math\n",
        "import base64\n",
        "import shutil\n",
        "import textwrap\n",
        "import subprocess\n",
        "import tempfile\n",
        "import zipfile\n",
        "import logging\n",
        "from dataclasses import dataclass, field\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, Iterable, List, Optional, Tuple\n",
        "from collections import Counter\n",
        "from html.parser import HTMLParser\n",
        "import requests\n",
        "try:\n",
        "    import tiktoken  # Lightweight token estimates\n",
        "    TIKTOKEN_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TIKTOKEN_AVAILABLE = False\n",
        "# --- Google/Colab Modules ---\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    from google.colab import userdata\n",
        "    COLAB_ENV = True\n",
        "except ImportError:\n",
        "    COLAB_ENV = False\n",
        "# --- Google GenAI SDK ---\n",
        "try:\n",
        "    from google import genai\n",
        "    from google.genai import types\n",
        "except ImportError:\n",
        "    print(\"Error: 'google-genai' library not found. Please run: !pip install google-genai\")\n",
        "    sys.exit(1)\n",
        "# --- GitHub SDK ---\n",
        "try:\n",
        "    from github import Github, GithubException\n",
        "except ImportError:\n",
        "    print(\"Error: 'PyGithub' library not found. Please run: !pip install PyGithub\")\n",
        "    sys.exit(1)\n"
      ],
      "id": "PE6PXwdp71ei"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWLk-Zkf71ei"
      },
      "source": [
        "## 2. Configuration\n",
        "\n",
        "Set cost limits, API models, and file paths here."
      ],
      "id": "KWLk-Zkf71ei"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TWSHF-pl71ei"
      },
      "outputs": [],
      "source": [
        "# --- Model Configuration ---\n",
        "# Using latest preview models as of Oct 2025\n",
        "GEN_MODEL_TEXT = \"gemini-2.5-flash-preview-09-2025\"\n",
        "# --- Cost Control Configuration ---\n",
        "# Prices per 1,000,000 tokens (as of late 2025 previews)\n",
        "# $0.35 / 1M input tokens, $0.70 / 1M output tokens for flash\n",
        "PRICE_FLASH_INPUT_PER_TOKEN = 0.35 / 1_000_000\n",
        "PRICE_FLASH_OUTPUT_PER_TOKEN = 0.70 / 1_000_000\n",
        "COST_LIMIT_USD = 25.00\n",
        "TOTAL_ESTIMATED_COST = 0.0  # Global cost tracker\n",
        "COST_LEDGER: List[Dict[str, Any]] = []  # Detailed log per task\n",
        "COST_REPORT_BASENAME = \"crew_cost_report.md\"\n",
        "# --- Google Drive / Local Output Configuration ---\n",
        "GDRIVE_MOUNT_POINT = '/content/drive'\n",
        "OUTPUT_FOLDER_NAME = 'colby_agentic_crew'\n",
        "if COLAB_ENV:\n",
        "    OUTPUT_ROOT_PATH = (Path(GDRIVE_MOUNT_POINT) / 'MyDrive' / OUTPUT_FOLDER_NAME).resolve()\n",
        "else:\n",
        "    OUTPUT_ROOT_PATH = (Path.cwd() / OUTPUT_FOLDER_NAME).resolve()\n",
        "CREW_TEMPLATE_DIR_NAME = 'crew_ai_cloudflare_template'\n",
        "COST_LOG_PATH = (OUTPUT_ROOT_PATH / COST_REPORT_BASENAME)\n",
        "# --- GitHub Configuration ---\n",
        "GITHUB_REPO = \"jmbish04/colby-agentic-crew\"\n",
        "GITHUB_SAVE_PATH = \"agent_generated_files\"  # Subfolder in the repo\n",
        "GITHUB_TOKEN = \"\"  # Will be loaded by init_clients\n",
        "# --- Cloudflare Org Discovery Configuration ---\n",
        "CLOUDFLARE_ORG = \"cloudflare\"\n",
        "REPO_SCAN_URL = f\"https://api.github.com/orgs/{CLOUDFLARE_ORG}/repos\"\n",
        "REPO_KEYWORDS = [\n",
        "    \"workers\",\n",
        "    \"durable\",\n",
        "    \"object\",\n",
        "    \"actor\",\n",
        "    \"agent\",\n",
        "    \"queue\",\n",
        "    \"workflow\",\n",
        "    \"sandbox\",\n",
        "    \"sdk\",\n",
        "    \"container\",\n",
        "]\n",
        "MAX_REPOS_TO_CLONE = 10\n",
        "REPO_DOWNLOAD_DIR = Path('/content/cloudflare_repos').resolve()\n",
        "MAX_INDEX_LINES = 20_000\n",
        "CONTEXT_MAX_CHARS = 8_000\n",
        "DRY_RUN = False  # When True, skip network heavy actions and model calls\n",
        "INDEX_SKIP_DIRS = {\n",
        "    '.git', '.github', 'node_modules', 'dist', 'build', 'out', 'vendor', '__pycache__', '.next'\n",
        "}\n",
        "BINARY_EXTENSIONS = {\n",
        "    '.png', '.jpg', '.jpeg', '.gif', '.webp', '.zip', '.tar', '.gz', '.lock', '.bin', '.exe', '.dll',\n",
        "    '.ico', '.wasm', '.woff', '.woff2', '.ttf', '.mp4', '.mp3'\n",
        "}\n",
        "LANGUAGE_HINTS = {\n",
        "    '.ts': 'TypeScript',\n",
        "    '.tsx': 'TypeScript',\n",
        "    '.js': 'JavaScript',\n",
        "    '.jsx': 'JavaScript',\n",
        "    '.py': 'Python',\n",
        "    '.rs': 'Rust',\n",
        "    '.go': 'Go',\n",
        "    '.toml': 'TOML',\n",
        "    '.json': 'JSON',\n",
        "    '.md': 'Markdown',\n",
        "    '.yml': 'YAML',\n",
        "    '.yaml': 'YAML',\n",
        "    '.sh': 'Shell',\n",
        "    '.tsconfig': 'JSON',\n",
        "}\n",
        "FRAMEWORK_KEYWORDS = {\n",
        "    'Hono': ['from \"hono\"', \"from 'hono'\", 'import { Hono }'],\n",
        "    'Wrangler': ['wrangler.toml', 'defineConfig(', 'wrangler dev'],\n",
        "    'Durable Objects': ['DurableObject', 'durable_objects'],\n",
        "    'Queues': ['QueuesBinding', 'queueProducer'],\n",
        "    'Workers AI': ['@cloudflare/workers-types', 'ai.run', 'Workers AI'],\n",
        "    'Workflows': ['workflows deploy', 'WorkflowsBinding'],\n",
        "}\n",
        "DEFAULT_SNIPPET_LINES = 20\n",
        "# --- Documentation Configuration ---\n",
        "CLOUDFLARE_DOC_TOPICS = {\n",
        "    'workers': 'https://developers.cloudflare.com/workers/',\n",
        "    'durable objects': 'https://developers.cloudflare.com/workers/runtime-apis/durable-objects/',\n",
        "    'queues': 'https://developers.cloudflare.com/queues/',\n",
        "    'd1': 'https://developers.cloudflare.com/d1/',\n",
        "    'r2': 'https://developers.cloudflare.com/r2/',\n",
        "    'kv': 'https://developers.cloudflare.com/workers/runtime-apis/kv/',\n",
        "    'agents': 'https://developers.cloudflare.com/workers/ai/agents/',\n",
        "    'workflows': 'https://developers.cloudflare.com/workflows/',\n",
        "    'wrangler': 'https://developers.cloudflare.com/workers/wrangler/configuration/',\n",
        "    'ai': 'https://developers.cloudflare.com/workers/ai/',\n",
        "}\n",
        "DOC_TOPICS_ORDER = [\n",
        "    'agents', 'durable objects', 'queues', 'workers', 'wrangler', 'r2', 'd1', 'kv', 'workflows', 'ai'\n",
        "]\n",
        "DOC_FETCH_TIMEOUT = 45  # seconds\n",
        "SUMMARY_FILENAME = 'cloudflare_repos_summary.json'\n",
        "CODE_INDEX_FILENAME = 'cloudflare_code_index.json'\n",
        "DOCS_REFERENCE_FILENAME = 'docs_reference.json'\n",
        "TIKTOKEN_MODEL_NAME = 'cl100k_base'\n"
      ],
      "id": "TWSHF-pl71ei"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMikfwMg71ej"
      },
      "source": [
        "## 3. Utilities\n",
        "\n",
        "Helper functions for reading secrets and API call backoff."
      ],
      "id": "gMikfwMg71ej"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cw72CVsk71ej"
      },
      "outputs": [],
      "source": [
        "def read_secret(name: str, prompt: Optional[str] = None) -> str:\n",
        "    \"\"\"\n",
        "    Read a secret from Colab userdata if available, else environment, else prompt stdin.\n",
        "    \"\"\"\n",
        "    if COLAB_ENV:\n",
        "        try:\n",
        "            val = userdata.get(name)\n",
        "            if val:\n",
        "                return val\n",
        "        except Exception:\n",
        "            pass\n",
        "    val = os.getenv(name)\n",
        "    if val:\n",
        "        return val\n",
        "    if 'getpass' not in sys.modules:\n",
        "        import getpass\n",
        "    if prompt is None:\n",
        "        prompt = f\"Enter value for {name}: \"\n",
        "    try:\n",
        "        return getpass.getpass(prompt)\n",
        "    except Exception:\n",
        "        return input(prompt)\n",
        "def backoff_sleep(retry: int):\n",
        "    delay = min(2 ** retry, 30) + (0.1 * retry)\n",
        "    print(f\"  ...backing off for {delay:.1f}s\")\n",
        "    time.sleep(delay)\n",
        "def timestamp_iso() -> str:\n",
        "    return datetime.utcnow().replace(microsecond=0).isoformat() + 'Z'\n",
        "class _HTMLStripper(HTMLParser):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self._parts: List[str] = []\n",
        "    def handle_data(self, data: str):\n",
        "        data = data.strip()\n",
        "        if data:\n",
        "            self._parts.append(data)\n",
        "    def get_text(self) -> str:\n",
        "        return ' '.join(self._parts)\n",
        "def strip_html(html_text: str) -> str:\n",
        "    stripper = _HTMLStripper()\n",
        "    try:\n",
        "        stripper.feed(html_text or '')\n",
        "    except Exception:\n",
        "        return html_text\n",
        "    return stripper.get_text()\n",
        "def ensure_directory(path: Path) -> Path:\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "    return path\n",
        "def write_json_document(path: Path, payload: Any) -> Path:\n",
        "    ensure_directory(path.parent)\n",
        "    path.write_text(json.dumps(payload, indent=2, sort_keys=False), encoding='utf-8')\n",
        "    print(f\"[json] Saved \u2192 {path}\")\n",
        "    return path\n",
        "def estimate_tokens(text: str) -> int:\n",
        "    if not text:\n",
        "        return 0\n",
        "    if TIKTOKEN_AVAILABLE:\n",
        "        try:\n",
        "            try:\n",
        "                encoding = tiktoken.get_encoding(TIKTOKEN_MODEL_NAME)\n",
        "            except Exception:\n",
        "                encoding = tiktoken.get_encoding('cl100k_base')\n",
        "            return len(encoding.encode(text))\n",
        "        except Exception as exc:\n",
        "            print(f\"[tiktoken] Warning: {exc}. Falling back to heuristic estimate.\")\n",
        "    return max(1, len(text) // 4)\n",
        "def truncate_for_context(text: str, limit: int = CONTEXT_MAX_CHARS) -> str:\n",
        "    if len(text) <= limit:\n",
        "        return text\n",
        "    head = text[: max(0, limit - 400)]\n",
        "    tail = text[-200:]\n",
        "    return head + \"\\n... [truncated] ...\\n\" + tail\n",
        "def is_binary_extension(path: Path) -> bool:\n",
        "    return path.suffix.lower() in BINARY_EXTENSIONS\n",
        "def should_skip_path(path: Path) -> bool:\n",
        "    return any((part in INDEX_SKIP_DIRS) or part.startswith('.') for part in path.parts)\n",
        "def safe_read_file_lines(path: Path, max_lines: int = DEFAULT_SNIPPET_LINES) -> Tuple[List[str], int]:\n",
        "    snippet: List[str] = []\n",
        "    total = 0\n",
        "    try:\n",
        "        with path.open('r', encoding='utf-8', errors='ignore') as handle:\n",
        "            for line in handle:\n",
        "                total += 1\n",
        "                if len(snippet) < max_lines:\n",
        "                    snippet.append(line.rstrip())\n",
        "    except Exception as exc:\n",
        "        snippet.append(f\"[Error reading file: {exc}]\")\n",
        "    return snippet, total\n"
      ],
      "id": "cw72CVsk71ej"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SA-mh3wC71ej"
      },
      "source": [
        "## 4. Cost Control\n",
        "\n",
        "Functions to track and enforce the `$25.00` spending limit."
      ],
      "id": "SA-mh3wC71ej"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9okq3gKK71ej"
      },
      "outputs": [],
      "source": [
        "\n",
        "def check_and_update_cost(\n",
        "    task_name: str,\n",
        "    prompt_tokens: int = 0,\n",
        "    output_tokens: int = 0,\n",
        "    model: str = GEN_MODEL_TEXT\n",
        ") -> bool:\n",
        "    \"\"\"\n",
        "    Checks if a new call would exceed the limit, logs the itemized cost,\n",
        "    and updates the global cost.\n",
        "    Returns True if the call can proceed, False if it's blocked.\n",
        "    \"\"\"\n",
        "    global TOTAL_ESTIMATED_COST, COST_LEDGER\n",
        "\n",
        "    call_cost = 0.0\n",
        "    if model == GEN_MODEL_TEXT:\n",
        "        call_cost += prompt_tokens * PRICE_FLASH_INPUT_PER_TOKEN\n",
        "        call_cost += output_tokens * PRICE_FLASH_OUTPUT_PER_TOKEN\n",
        "\n",
        "    details = f\"{prompt_tokens} in, {output_tokens} out\"\n",
        "    projected_total = TOTAL_ESTIMATED_COST + call_cost\n",
        "\n",
        "    if projected_total > COST_LIMIT_USD:\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"COST LIMIT BREACHED: Task '{task_name}' cannot proceed.\")\n",
        "        print(f\"  Current Cost: ${TOTAL_ESTIMATED_COST:.4f}\")\n",
        "        print(f\"  Attempted Call Cost: ${call_cost:.4f}\")\n",
        "        print(f\"  Limit: ${COST_LIMIT_USD:.4f}\")\n",
        "        print(\"=\" * 50)\n",
        "        COST_LEDGER.append({\n",
        "            \"timestamp\": timestamp_iso(),\n",
        "            \"task\": f\"FAILED: {task_name} (Cost Limit Breach)\",\n",
        "            \"status\": \"blocked\",\n",
        "            \"prompt_tokens\": prompt_tokens,\n",
        "            \"output_tokens\": output_tokens,\n",
        "            \"cost\": 0.0,\n",
        "            \"cumulative_cost\": TOTAL_ESTIMATED_COST,\n",
        "            \"details\": details,\n",
        "        })\n",
        "        return False\n",
        "\n",
        "    TOTAL_ESTIMATED_COST = projected_total\n",
        "    entry = {\n",
        "        \"timestamp\": timestamp_iso(),\n",
        "        \"task\": task_name,\n",
        "        \"status\": \"ok\",\n",
        "        \"prompt_tokens\": prompt_tokens,\n",
        "        \"output_tokens\": output_tokens,\n",
        "        \"cost\": call_cost,\n",
        "        \"cumulative_cost\": TOTAL_ESTIMATED_COST,\n",
        "        \"details\": details,\n",
        "    }\n",
        "    COST_LEDGER.append(entry)\n",
        "    print(f\"[CostTracker] Task '{task_name}' \u2192 ${call_cost:.6f} (cumulative ${TOTAL_ESTIMATED_COST:.6f})\")\n",
        "    return True\n",
        "\n",
        "\n",
        "def format_cost_ledger_markdown() -> str:\n",
        "    lines = [\n",
        "        \"# Crew Cost Ledger\",\n",
        "        \"\",\n",
        "        \"| Timestamp | Task | Status | Prompt Tokens | Output Tokens | Cost (USD) | Cumulative (USD) | Details |\",\n",
        "        \"| :-- | :-- | :-- | --: | --: | --: | --: | :-- |\",\n",
        "    ]\n",
        "    for item in COST_LEDGER:\n",
        "        lines.append(\n",
        "            f\"| {item.get('timestamp', '')} | {item.get('task', '')} | {item.get('status', '')} | \"\n",
        "            f\"{item.get('prompt_tokens', 0)} | {item.get('output_tokens', 0)} | ${item.get('cost', 0.0):.6f} | \"\n",
        "            f\"${item.get('cumulative_cost', 0.0):.6f} | {item.get('details', '')} |\"\n",
        "        )\n",
        "    lines.append(\"\")\n",
        "    lines.append(\"*Log generated automatically by Colby Agentic Crew.*\")\n",
        "    return \"\n",
        "\".join(lines)\n",
        "\n",
        "\n",
        "def persist_cost_ledger() -> Optional[Path]:\n",
        "    if COST_LOG_PATH is None:\n",
        "        return None\n",
        "    ensure_directory(Path(COST_LOG_PATH).parent)\n",
        "    content = format_cost_ledger_markdown()\n",
        "    Path(COST_LOG_PATH).write_text(content, encoding=\"utf-8\")\n",
        "    print(f\"[cost] Ledger saved \u2192 {COST_LOG_PATH}\")\n",
        "    return Path(COST_LOG_PATH)\n",
        "\n"
      ],
      "id": "9okq3gKK71ej"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cloudflareDocsHeading"
      },
      "source": [
        "## 5. Cloudflare Documentation Client\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cloudflareDocsClient"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "class CloudflareMCPClient:\n",
        "    \"\"\"Simple client to pull reference excerpts from Cloudflare documentation.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        session: Optional[requests.Session] = None,\n",
        "        doc_topics: Optional[Dict[str, str]] = None,\n",
        "    ) -> None:\n",
        "        self.session = session or requests.Session()\n",
        "        self.session.headers.setdefault(\"User-Agent\", \"colby-agentic-crew/1.0\")\n",
        "        topic_map = doc_topics or CLOUDFLARE_DOC_TOPICS\n",
        "        self.doc_topics = {key.lower(): url for key, url in topic_map.items()}\n",
        "\n",
        "    def _resolve_topic(self, topic: str) -> Tuple[str, Optional[str]]:\n",
        "        key = topic.lower().strip()\n",
        "        if key in self.doc_topics:\n",
        "            return key, self.doc_topics[key]\n",
        "        for known_key, url in self.doc_topics.items():\n",
        "            if key in known_key:\n",
        "                return known_key, url\n",
        "        return key, None\n",
        "\n",
        "    def fetch_topic(self, topic: str) -> Dict[str, Any]:\n",
        "        resolved_key, url = self._resolve_topic(topic)\n",
        "        if not url:\n",
        "            return {\n",
        "                \"topic\": topic,\n",
        "                \"status\": \"missing\",\n",
        "                \"url\": None,\n",
        "                \"excerpt\": \"\",\n",
        "            }\n",
        "        try:\n",
        "            resp = self.session.get(url, timeout=DOC_FETCH_TIMEOUT)\n",
        "            resp.raise_for_status()\n",
        "            text = strip_html(resp.text)\n",
        "            excerpt = textwrap.shorten(text, width=1500, placeholder=\"\u2026\")\n",
        "            return {\n",
        "                \"topic\": resolved_key,\n",
        "                \"status\": \"ok\",\n",
        "                \"url\": url,\n",
        "                \"excerpt\": excerpt,\n",
        "            }\n",
        "        except Exception as exc:\n",
        "            return {\n",
        "                \"topic\": resolved_key,\n",
        "                \"status\": \"error\",\n",
        "                \"url\": url,\n",
        "                \"excerpt\": f\"Error fetching doc: {exc}\",\n",
        "            }\n",
        "\n",
        "    def gather_docs(self, topics: Optional[List[str]] = None) -> Dict[str, Any]:\n",
        "        chosen_topics = topics or list(self.doc_topics.keys())\n",
        "        documents = [self.fetch_topic(topic) for topic in chosen_topics]\n",
        "        return {\n",
        "            \"queried_at\": timestamp_iso(),\n",
        "            \"topics\": chosen_topics,\n",
        "            \"documents\": documents,\n",
        "        }\n",
        "\n",
        "    def summarize_for_prompt(self, docs_payload: Dict[str, Any], limit: int = 4000) -> str:\n",
        "        lines: List[str] = []\n",
        "        for doc in docs_payload.get(\"documents\", []):\n",
        "            status = doc.get(\"status\")\n",
        "            topic = doc.get(\"topic\", \"unknown\").title()\n",
        "            url = doc.get(\"url\") or \"\"\n",
        "            if status != \"ok\":\n",
        "                lines.append(f\"- {topic}: {status} ({url})\")\n",
        "                continue\n",
        "            excerpt = textwrap.shorten(doc.get(\"excerpt\", \"\"), width=400, placeholder=\"\u2026\")\n",
        "            lines.append(f\"- {topic}: {url}\n",
        "  Excerpt: {excerpt}\")\n",
        "        summary = \"\n",
        "\".join(lines)\n",
        "        return truncate_for_context(summary, limit=limit)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfwBjZH771ej"
      },
      "source": [
        "## 6. API Clients\n",
        "\n"
      ],
      "id": "LfwBjZH771ej"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GGAlV5v_71ej"
      },
      "outputs": [],
      "source": [
        "\n",
        "@dataclass\n",
        "class Clients:\n",
        "    genai_client: genai.Client\n",
        "    gh: Github\n",
        "    docs_client: CloudflareMCPClient\n",
        "\n",
        "\n",
        "def init_clients() -> Optional[Clients]:\n",
        "    \"\"\"Initializes and returns all API clients.\"\"\"\n",
        "    global GITHUB_TOKEN\n",
        "    try:\n",
        "        print(\"Initializing clients...\")\n",
        "        gemini_key = read_secret(\"GOOGLE_API_KEY\", \"GOOGLE_API_KEY (Google API key): \")\n",
        "        GITHUB_TOKEN = read_secret(\"GITHUB_TOKEN\", \"GITHUB_TOKEN (GitHub PAT): \")\n",
        "\n",
        "        if not gemini_key:\n",
        "            print(\"ERROR: GOOGLE_API_KEY is not set in Colab secrets (View > Show secrets).\")\n",
        "            return None\n",
        "\n",
        "        if not GITHUB_TOKEN:\n",
        "            print(\"ERROR: GITHUB_TOKEN is not set in Colab secrets (View > Show secrets).\")\n",
        "            return None\n",
        "\n",
        "        genai_client = genai.Client(api_key=gemini_key)\n",
        "        gh = Github(GITHUB_TOKEN, per_page=50)\n",
        "        _ = gh.get_user().login\n",
        "        print(\"GitHub client authenticated.\")\n",
        "\n",
        "        docs_client = CloudflareMCPClient()\n",
        "        print(\"Documentation client ready.\")\n",
        "\n",
        "        print(\"All clients initialized successfully.\")\n",
        "        return Clients(genai_client, gh, docs_client)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing clients: {e}\")\n",
        "        if 'Bad credentials' in str(e):\n",
        "            print(\"Hint: Check your GITHUB_TOKEN.\")\n",
        "        return None\n",
        "\n"
      ],
      "id": "GGAlV5v_71ej"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2LLMKu771ej"
      },
      "source": [
        "## 7. Google GenAI Operations\n",
        "\n"
      ],
      "id": "L2LLMKu771ej"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "j8NjNNrY71ej"
      },
      "outputs": [],
      "source": [
        "\n",
        "def gen_text(\n",
        "    clients: Clients,\n",
        "    task_name: str,\n",
        "    prompt: str,\n",
        "    max_retries: int = 3,\n",
        "    system_instruction: Optional[str] = None,\n",
        "    additional_context: Optional[str] = None,\n",
        ") -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Generate text content using google.genai (Gemini Developer API via API key).\n",
        "    Allows injecting additional context into the system instruction and\n",
        "    tracks cost usage.\n",
        "    \"\"\"\n",
        "    global TOTAL_ESTIMATED_COST\n",
        "\n",
        "    default_system = (\n",
        "        \"You are an expert AI assistant specializing in Cloudflare Workers, \"\n",
        "        \"agentic systems, and CrewAI. Provide complete, production-ready code and configurations.\"\n",
        "    )\n",
        "    system_text = system_instruction or default_system\n",
        "    if additional_context:\n",
        "        system_text = f\"{system_text}\n",
        "\n",
        "Context:\n",
        "{additional_context}\"\n",
        "\n",
        "    print(f\"\n",
        "--- Starting Task: {task_name} ---\")\n",
        "    print(f\"Generating text for prompt: '{prompt[:50]}...'\")\n",
        "\n",
        "    contents = prompt\n",
        "\n",
        "    try:\n",
        "        ct = clients.genai_client.models.count_tokens(\n",
        "            model=GEN_MODEL_TEXT,\n",
        "            contents=contents,\n",
        "            config=types.GenerateContentConfig(\n",
        "                system_instruction=system_text,\n",
        "            ),\n",
        "        )\n",
        "        prompt_tokens = getattr(ct, \"total_tokens\", None) or 0\n",
        "        print(f\"[CostTracker] Estimated prompt tokens: {prompt_tokens}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not pre-count tokens via API: {e}. Using local estimator.\")\n",
        "        prompt_tokens = estimate_tokens(system_text) + estimate_tokens(prompt)\n",
        "\n",
        "    pre_check_cost = prompt_tokens * PRICE_FLASH_INPUT_PER_TOKEN\n",
        "    if (TOTAL_ESTIMATED_COST + pre_check_cost) > COST_LIMIT_USD:\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"COST LIMIT BREACH: Pre-check for task '{task_name}' failed.\")\n",
        "        print(f\"  Current Cost: ${TOTAL_ESTIMATED_COST:.4f}\")\n",
        "        print(f\"  Est. Prompt Cost: ${pre_check_cost:.4f}\")\n",
        "        print(f\"  Limit: ${COST_LIMIT_USD:.4f}\")\n",
        "        print(\"=\" * 50)\n",
        "        COST_LEDGER.append({\n",
        "            \"timestamp\": timestamp_iso(),\n",
        "            \"task\": f\"SKIPPED: {task_name} (Pre-check failed cost limit)\",\n",
        "            \"status\": \"blocked\",\n",
        "            \"prompt_tokens\": prompt_tokens,\n",
        "            \"output_tokens\": 0,\n",
        "            \"cost\": 0.0,\n",
        "            \"cumulative_cost\": TOTAL_ESTIMATED_COST,\n",
        "            \"details\": f\"Pre-estimate: {prompt_tokens} tokens\",\n",
        "        })\n",
        "        return None\n",
        "\n",
        "    for attempt in range(max_retries + 1):\n",
        "        try:\n",
        "            resp = clients.genai_client.models.generate_content(\n",
        "                model=GEN_MODEL_TEXT,\n",
        "                contents=contents,\n",
        "                config=types.GenerateContentConfig(\n",
        "                    system_instruction=system_text,\n",
        "                    temperature=0.3,\n",
        "                    top_p=0.9,\n",
        "                    max_output_tokens=2048,\n",
        "                ),\n",
        "            )\n",
        "\n",
        "            usage = getattr(resp, \"usage_metadata\", None)\n",
        "            if usage:\n",
        "                in_tokens = getattr(usage, \"prompt_token_count\", 0)\n",
        "                out_tokens = getattr(usage, \"candidates_token_count\", 0)\n",
        "                ok = check_and_update_cost(\n",
        "                    task_name=task_name,\n",
        "                    prompt_tokens=in_tokens,\n",
        "                    output_tokens=out_tokens,\n",
        "                    model=GEN_MODEL_TEXT,\n",
        "                )\n",
        "                if not ok:\n",
        "                    return \"(Cost limit reached during generation)\"\n",
        "            else:\n",
        "                print(\"Warning: usage_metadata missing; relying on local estimate.\")\n",
        "                check_and_update_cost(\n",
        "                    task_name=f\"{task_name} (Usage data missing)\",\n",
        "                    prompt_tokens=prompt_tokens,\n",
        "                    output_tokens=0,\n",
        "                    model=GEN_MODEL_TEXT,\n",
        "                )\n",
        "\n",
        "            text_out = getattr(resp, \"text\", \"\") or \"\"\n",
        "            if not text_out:\n",
        "                parts = []\n",
        "                for cand in getattr(resp, \"candidates\", []) or []:\n",
        "                    for part in getattr(cand.content, \"parts\", []) or []:\n",
        "                        if getattr(part, \"text\", None):\n",
        "                            parts.append(part.text)\n",
        "                text_out = \"\n",
        "\".join(parts).strip()\n",
        "\n",
        "            if not text_out:\n",
        "                raise RuntimeError(\"No text output received from model.\")\n",
        "\n",
        "            print(\"Text generation successful.\")\n",
        "            return text_out\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"GenAI text error (Attempt {attempt + 1}): {e}\")\n",
        "            if attempt >= max_retries:\n",
        "                raise\n",
        "            backoff_sleep(attempt)\n",
        "\n",
        "    return None\n",
        "\n"
      ],
      "id": "j8NjNNrY71ej"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUOZhdtA71ek"
      },
      "source": [
        "## 8. GitHub Operations (SDK)\n",
        "\n"
      ],
      "id": "gUOZhdtA71ek"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "fBKSXkU-71ek"
      },
      "outputs": [],
      "source": [
        "from typing import List, Optional, cast\n",
        "from github import Github\n",
        "from github.GithubException import GithubException\n",
        "from github.Repository import Repository\n",
        "from github.Issue import Issue\n",
        "from github.PaginatedList import PaginatedList\n",
        "\n",
        "def gh_get_repo(gh: Github, full_name: str) -> Repository:\n",
        "    \"\"\"Get a repo like 'owner/name'. Raises if not found or no access.\"\"\"\n",
        "    try:\n",
        "        repo = gh.get_repo(full_name)\n",
        "        return cast(Repository, repo)\n",
        "    except GithubException as e:\n",
        "        print(f\"GitHub repo error for {full_name}: {getattr(e, 'data', e)}\")\n",
        "        raise\n",
        "\n",
        "def gh_list_issues(gh: Github, repo_full_name: str, state: str = \"open\", limit: int = 5):\n",
        "    print(f\"Listing last {limit} '{state}' issues for {repo_full_name}...\")\n",
        "    repo = gh_get_repo(gh, repo_full_name)\n",
        "\n",
        "    issues_pl = repo.get_issues(state=state)\n",
        "    issues_pl = cast(PaginatedList, issues_pl)  # PaginatedList[Issue] at runtime\n",
        "\n",
        "    out: List[dict] = []\n",
        "    # safer than slicing a PaginatedList for some stub checkers\n",
        "    for i, issue in enumerate(issues_pl):\n",
        "        if i >= limit:\n",
        "            break\n",
        "        issue = cast(Issue, issue)\n",
        "        out.append({\n",
        "            \"number\": issue.number,\n",
        "            \"title\": issue.title,\n",
        "            \"state\": issue.state,\n",
        "            \"url\": issue.html_url,\n",
        "        })\n",
        "    print(f\"Found {len(out)} issues.\")\n",
        "    return out\n",
        "\n",
        "def gh_create_issue(gh: Github, repo_full_name: str, title: str, body: str = \"\", labels: Optional[List[str]] = None) -> int:\n",
        "    print(f\"Creating issue in {repo_full_name}: '{title}'\")\n",
        "    repo = gh_get_repo(gh, repo_full_name)\n",
        "    issue = repo.create_issue(title=title, body=body, labels=labels or [])\n",
        "    issue = cast(Issue, issue)\n",
        "    print(f\"Issue #{issue.number} created successfully.\")\n",
        "    return issue.number\n",
        "\n",
        "def gh_comment_issue(gh: Github, repo_full_name: str, number: int, body: str) -> str:\n",
        "    print(f\"Commenting on issue #{number} in {repo_full_name}...\")\n",
        "    repo = gh_get_repo(gh, repo_full_name)\n",
        "    issue = cast(Issue, repo.get_issue(number=number))\n",
        "    comment = issue.create_comment(body)\n",
        "    # comment is a GithubObject; just use getattr to keep pyright happy\n",
        "    url = getattr(comment, \"html_url\", \"\")\n",
        "    print(f\"Comment posted: {url}\")\n",
        "    return url"
      ],
      "id": "fBKSXkU-71ek"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cloudflareRepoHeading"
      },
      "source": [
        "## 9. Cloudflare Org Discovery & Indexing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cloudflareRepoCode"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "def normalize_keywords(keywords: Iterable[str]) -> List[str]:\n",
        "    return sorted({kw.lower().strip() for kw in keywords if kw})\n",
        "def repo_keyword_matches(repo: Dict[str, Any], keywords: List[str]) -> List[str]:\n",
        "    haystack = \" \".join([\n",
        "        repo.get(\"name\", \"\"),\n",
        "        repo.get(\"description\", \"\") or \"\",\n",
        "        \" \".join(repo.get(\"topics\", []) or []),\n",
        "    ]).lower()\n",
        "    return [kw for kw in keywords if kw in haystack]\n",
        "def fetch_cloudflare_repositories(keywords: Iterable[str]) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "    keyword_list = normalize_keywords(keywords)\n",
        "    session = requests.Session()\n",
        "    session.headers.update(_gh_headers())\n",
        "    relevant: List[Dict[str, Any]] = []\n",
        "    total_scanned = 0\n",
        "    page = 1\n",
        "    while True:\n",
        "        params = {\"per_page\": 100, \"page\": page}\n",
        "        resp = session.get(REPO_SCAN_URL, params=params, timeout=60)\n",
        "        if resp.status_code in (429, 502, 503, 504):\n",
        "            backoff_sleep(page)\n",
        "            continue\n",
        "        resp.raise_for_status()\n",
        "        data = resp.json()\n",
        "        if not data:\n",
        "            break\n",
        "        total_scanned += len(data)\n",
        "        for repo in data:\n",
        "            matches = repo_keyword_matches(repo, keyword_list)\n",
        "            entry = {\n",
        "                \"name\": repo.get(\"name\"),\n",
        "                \"full_name\": repo.get(\"full_name\"),\n",
        "                \"url\": repo.get(\"html_url\"),\n",
        "                \"description\": repo.get(\"description\"),\n",
        "                \"stars\": repo.get(\"stargazers_count\", 0),\n",
        "                \"topics\": repo.get(\"topics\", []),\n",
        "                \"pushed_at\": repo.get(\"pushed_at\"),\n",
        "                \"matches\": matches,\n",
        "                \"clone_url\": repo.get(\"clone_url\"),\n",
        "                \"archive_url\": repo.get(\"archive_url\"),\n",
        "                \"default_branch\": repo.get(\"default_branch\", \"main\"),\n",
        "            }\n",
        "            if matches:\n",
        "                relevant.append(entry)\n",
        "        page += 1\n",
        "        if 'next' not in resp.links:\n",
        "            break\n",
        "    summary = {\n",
        "        \"queried_at\": timestamp_iso(),\n",
        "        \"total_scanned\": total_scanned,\n",
        "        \"relevant_repos\": relevant,\n",
        "    }\n",
        "    return relevant, summary\n",
        "def score_repository(repo: Dict[str, Any]) -> Tuple[int, int, str]:\n",
        "    return (len(repo.get(\"matches\", [])), repo.get(\"stars\", 0), repo.get(\"name\", \"\"))\n",
        "def select_top_repositories(repos: List[Dict[str, Any]], limit: int = MAX_REPOS_TO_CLONE) -> List[Dict[str, Any]]:\n",
        "    return sorted(repos, key=score_repository, reverse=True)[:limit]\n",
        "def _clone_repository(repo: Dict[str, Any], destination: Path) -> bool:\n",
        "    ensure_directory(destination.parent)\n",
        "    if destination.exists():\n",
        "        shutil.rmtree(destination)\n",
        "    clone_url = repo.get(\"clone_url\")\n",
        "    if clone_url:\n",
        "        try:\n",
        "            subprocess.run(\n",
        "                [\"git\", \"clone\", \"--depth\", \"1\", clone_url, str(destination)],\n",
        "                check=True,\n",
        "                stdout=subprocess.PIPE,\n",
        "                stderr=subprocess.PIPE,\n",
        "            )\n",
        "            return True\n",
        "        except Exception as exc:\n",
        "            print(f\"[clone] git clone failed for {repo.get('name')}: {exc}\")\n",
        "    # Fallback to zip download\n",
        "    try:\n",
        "        full_name = repo.get(\"full_name\")\n",
        "        branch = repo.get(\"default_branch\", \"main\")\n",
        "        if not full_name:\n",
        "            return False\n",
        "        zip_url = f\"https://api.github.com/repos/{full_name}/zipball/{branch}\"\n",
        "        resp = requests.get(zip_url, headers=_gh_headers(), timeout=120)\n",
        "        resp.raise_for_status()\n",
        "        with tempfile.TemporaryDirectory() as tmpdir:\n",
        "            zip_path = Path(tmpdir) / \"repo.zip\"\n",
        "            zip_path.write_bytes(resp.content)\n",
        "            with zipfile.ZipFile(zip_path) as zf:\n",
        "                zf.extractall(tmpdir)\n",
        "            extracted_dirs = [p for p in Path(tmpdir).iterdir() if p.is_dir()]\n",
        "            if not extracted_dirs:\n",
        "                return False\n",
        "            extracted_root = extracted_dirs[0]\n",
        "            if destination.exists():\n",
        "                shutil.rmtree(destination)\n",
        "            shutil.move(str(extracted_root), destination)\n",
        "        return True\n",
        "    except Exception as exc:\n",
        "        print(f\"[clone] zip fallback failed for {repo.get('name')}: {exc}\")\n",
        "        return False\n",
        "def download_repositories(repos: List[Dict[str, Any]], dry_run: bool = False) -> List[Dict[str, Any]]:\n",
        "    ensure_directory(REPO_DOWNLOAD_DIR)\n",
        "    downloaded: List[Dict[str, Any]] = []\n",
        "    for repo in repos:\n",
        "        target_path = REPO_DOWNLOAD_DIR / repo.get(\"name\", \"repo\")\n",
        "        if dry_run:\n",
        "            ensure_directory(target_path)\n",
        "            downloaded.append({**repo, \"local_path\": str(target_path)})\n",
        "            continue\n",
        "        success = _clone_repository(repo, target_path)\n",
        "        if success:\n",
        "            downloaded.append({**repo, \"local_path\": str(target_path)})\n",
        "    return downloaded\n",
        "def detect_language(path: Path) -> str:\n",
        "    suffix = path.suffix.lower()\n",
        "    if suffix in LANGUAGE_HINTS:\n",
        "        return LANGUAGE_HINTS[suffix]\n",
        "    if suffix:\n",
        "        return suffix.lstrip('.') or 'Text'\n",
        "    return 'Text'\n",
        "def index_repositories(downloaded: List[Dict[str, Any]], max_lines: int = MAX_INDEX_LINES) -> Dict[str, Any]:\n",
        "    repo_entries: List[Dict[str, Any]] = []\n",
        "    remaining_lines = max_lines\n",
        "    total_indexed = 0\n",
        "    for repo in downloaded:\n",
        "        repo_path = Path(repo.get(\"local_path\", \"\"))\n",
        "        if not repo_path.exists():\n",
        "            continue\n",
        "        languages: Counter[str] = Counter()\n",
        "        key_files: List[str] = []\n",
        "        framework_hits: set[str] = set()\n",
        "        file_samples: List[Dict[str, Any]] = []\n",
        "        repo_lines = 0\n",
        "        top_level_dirs = [\n",
        "            item.name for item in repo_path.iterdir()\n",
        "            if item.is_dir() and item.name not in INDEX_SKIP_DIRS and not item.name.startswith('.')\n",
        "        ]\n",
        "        for path in sorted(repo_path.rglob('*')):\n",
        "            if remaining_lines <= 0:\n",
        "                break\n",
        "            if path.is_dir():\n",
        "                continue\n",
        "            rel_path = path.relative_to(repo_path)\n",
        "            if should_skip_path(rel_path):\n",
        "                continue\n",
        "            if path.name.startswith('.'):\n",
        "                continue\n",
        "            if is_binary_extension(path):\n",
        "                continue\n",
        "            language = detect_language(path)\n",
        "            languages[language] += 1\n",
        "            if rel_path.name in {\"wrangler.toml\", \"README.md\", \"package.json\", \"tsconfig.json\"}:\n",
        "                key_files.append(str(rel_path).replace('\\\\', '/'))\n",
        "            snippet, line_count = safe_read_file_lines(path, DEFAULT_SNIPPET_LINES)\n",
        "            consumed = min(line_count, remaining_lines)\n",
        "            remaining_lines -= consumed\n",
        "            total_indexed += consumed\n",
        "            repo_lines += consumed\n",
        "            snippet_text = \"\n",
        "\".join(snippet)\n",
        "            for framework, patterns in FRAMEWORK_KEYWORDS.items():\n",
        "                if any(pattern in snippet_text for pattern in patterns):\n",
        "                    framework_hits.add(framework)\n",
        "            file_samples.append({\n",
        "                \"path\": str(rel_path).replace('\\\\', '/'),\n",
        "                \"language\": language,\n",
        "                \"line_count\": line_count,\n",
        "                \"snippet\": snippet_text,\n",
        "            })\n",
        "            if len(file_samples) >= 25:\n",
        "                continue\n",
        "        repo_entry = {\n",
        "            \"name\": repo.get(\"name\"),\n",
        "            \"url\": repo.get(\"url\"),\n",
        "            \"description\": repo.get(\"description\"),\n",
        "            \"stars\": repo.get(\"stars\"),\n",
        "            \"matches\": repo.get(\"matches\", []),\n",
        "            \"key_directories\": sorted(top_level_dirs),\n",
        "            \"key_files\": sorted(set(key_files)),\n",
        "            \"languages\": [f\"{lang} ({count})\" for lang, count in languages.most_common()],\n",
        "            \"frameworks\": sorted(framework_hits),\n",
        "            \"samples\": file_samples[:25],\n",
        "            \"lines_indexed\": repo_lines,\n",
        "        }\n",
        "        repo_entries.append(repo_entry)\n",
        "        if remaining_lines <= 0:\n",
        "            break\n",
        "    return {\n",
        "        \"indexed_at\": timestamp_iso(),\n",
        "        \"max_total_lines\": max_lines,\n",
        "        \"total_lines_indexed\": total_indexed,\n",
        "        \"repos\": repo_entries,\n",
        "    }\n",
        "def build_repo_context(summary: Dict[str, Any], index: Dict[str, Any]) -> str:\n",
        "    lines: List[str] = [\n",
        "        f\"Cloudflare org scan completed at {summary.get('queried_at')}.\",\n",
        "        f\"Total repositories scanned: {summary.get('total_scanned', 0)}.\",\n",
        "        f\"Relevant repositories indexed: {len(index.get('repos', []))}.\",\n",
        "        \"\",\n",
        "    ]\n",
        "    for repo in index.get(\"repos\", [])[:10]:\n",
        "        lines.append(\n",
        "            f\"- {repo.get('name')} ({repo.get('stars', 0)}\u2605): {repo.get('description') or 'No description.'}\"\n",
        "        )\n",
        "        if repo.get(\"languages\"):\n",
        "            lines.append(f\"  Languages: {', '.join(repo['languages'][:5])}\")\n",
        "        if repo.get(\"frameworks\"):\n",
        "            lines.append(f\"  Framework signals: {', '.join(repo['frameworks'])}\")\n",
        "        if repo.get(\"matches\"):\n",
        "            lines.append(f\"  Keyword matches: {', '.join(repo['matches'])}\")\n",
        "        if repo.get(\"key_files\"):\n",
        "            lines.append(f\"  Key files: {', '.join(repo['key_files'][:5])}\")\n",
        "    context = \"\n",
        "\".join(lines)\n",
        "    return truncate_for_context(context, limit=CONTEXT_MAX_CHARS)\n",
        "def prepare_cloudflare_assets(keywords: Iterable[str], dry_run: bool = False) -> Dict[str, Any]:\n",
        "    relevant, summary = fetch_cloudflare_repositories(keywords)\n",
        "    top_repos = select_top_repositories(relevant, MAX_REPOS_TO_CLONE)\n",
        "    downloads = download_repositories(top_repos, dry_run=dry_run)\n",
        "    code_index = index_repositories(downloads, MAX_INDEX_LINES)\n",
        "    context = build_repo_context(summary, code_index)\n",
        "    return {\n",
        "        \"summary\": summary,\n",
        "        \"top_repos\": top_repos,\n",
        "        \"downloads\": downloads,\n",
        "        \"code_index\": code_index,\n",
        "        \"context\": context,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUlxHv6t71ek"
      },
      "source": [
        "## 10. Google Drive & GitHub File Ops (REST API)\n",
        "\n"
      ],
      "id": "bUlxHv6t71ek"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "P5aiRU9Z71ek"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ========== Google Drive (mounted) ==========\n",
        "def ensure_drive_ready() -> bool:\n",
        "    \"\"\"Mounts Drive (if in Colab) and ensures the output folder exists.\"\"\"\n",
        "    try:\n",
        "        if COLAB_ENV:\n",
        "            print(\"[drive] Mounting\u2026\")\n",
        "            drive.mount(GDRIVE_MOUNT_POINT)\n",
        "        ensure_directory(OUTPUT_ROOT_PATH)\n",
        "        print(f\"[drive] Ready: {OUTPUT_ROOT_PATH}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"[drive] ERROR preparing output: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "def save_to_drive(rel_path: str, content: str | bytes, binary: bool = False) -> str:\n",
        "    \"\"\"\n",
        "    Saves a file under the configured output folder.\n",
        "    rel_path: path relative to OUTPUT_ROOT_PATH (e.g., 'plans/wrangler.toml').\n",
        "    content:  str or bytes. Set binary=True if bytes.\n",
        "    \"\"\"\n",
        "    target_root = ensure_directory(OUTPUT_ROOT_PATH)\n",
        "    abs_path = (target_root / rel_path).resolve()\n",
        "    ensure_directory(abs_path.parent)\n",
        "\n",
        "    mode = \"wb\" if binary else \"w\"\n",
        "    encoding = None if binary else \"utf-8\"\n",
        "    with open(abs_path, mode, encoding=encoding) as handle:\n",
        "        handle.write(content)\n",
        "    print(f\"[drive] Saved \u2192 {abs_path}\")\n",
        "    return str(abs_path)\n",
        "\n",
        "\n",
        "# ========== GitHub (Contents API) ==========\n",
        "_GH_API = \"https://api.github.com\"\n",
        "\n",
        "def _gh_headers() -> dict:\n",
        "    if not GITHUB_TOKEN:\n",
        "        raise RuntimeError(\"GITHUB_TOKEN is missing (set in Colab secrets or env).\")\n",
        "    return {\n",
        "        \"Authorization\": f\"Bearer {GITHUB_TOKEN}\",\n",
        "        \"Accept\": \"application/vnd.github+json\",\n",
        "        \"X-GitHub-Api-Version\": \"2022-11-28\",\n",
        "        \"User-Agent\": \"genai-colab-uploader\",\n",
        "    }\n",
        "\n",
        "def _gh_get_default_branch(repo_full: str) -> str:\n",
        "    r = requests.get(f\"{_GH_API}/repos/{repo_full}\", headers=_gh_headers(), timeout=30)\n",
        "    r.raise_for_status()\n",
        "    return r.json().get(\"default_branch\", \"main\")\n",
        "\n",
        "def _gh_get_file_sha(repo_full: str, path: str, ref: Optional[str] = None) -> Optional[str]:\n",
        "    params = {\"ref\": ref} if ref else {}\n",
        "    r = requests.get(f\"{_GH_API}/repos/{repo_full}/contents/{path}\", headers=_gh_headers(), params=params, timeout=30)\n",
        "    if r.status_code == 404:\n",
        "        return None\n",
        "    r.raise_for_status()\n",
        "    return r.json().get(\"sha\")\n",
        "\n",
        "def _gh_put_contents(repo_full: str, path: str, message: str, b64content: str, branch: Optional[str], sha: Optional[str]):\n",
        "    payload = {\"message\": message, \"content\": b64content}\n",
        "    if branch:\n",
        "        payload[\"branch\"] = branch\n",
        "    if sha:\n",
        "        payload[\"sha\"] = sha\n",
        "    r = requests.put(f\"{_GH_API}/repos/{repo_full}/contents/{path}\", headers=_gh_headers(), json=payload, timeout=60)\n",
        "    if r.status_code >= 400:\n",
        "        try:\n",
        "            print(\"[github] Error payload:\", r.json())\n",
        "        except Exception:\n",
        "            print(\"[github] Error text:\", r.text)\n",
        "    r.raise_for_status()\n",
        "    return r.json()\n",
        "\n",
        "def save_to_github(\n",
        "    path: str,\n",
        "    content: str | bytes,\n",
        "    commit_message: str,\n",
        "    repo_full: str = GITHUB_REPO,\n",
        "    branch: Optional[str] = None,\n",
        "    max_retries: int = 3\n",
        ") -> dict:\n",
        "    if branch is None:\n",
        "        branch = _gh_get_default_branch(repo_full)\n",
        "\n",
        "    content_bytes = content if isinstance(content, (bytes, bytearray)) else content.encode(\"utf-8\")\n",
        "    b64 = base64.b64encode(content_bytes).decode(\"ascii\")\n",
        "\n",
        "    sha = _gh_get_file_sha(repo_full, path, ref=branch)\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            print(f\"[github] PUT {repo_full}:{branch}/{path} (attempt {attempt + 1})\")\n",
        "            return _gh_put_contents(repo_full, path, commit_message, b64, branch, sha)\n",
        "        except requests.HTTPError as e:\n",
        "            code = getattr(e.response, \"status_code\", None)\n",
        "            if code in (429, 502, 503, 504) and attempt < max_retries - 1:\n",
        "                delay = min(2 ** attempt, 30) + 0.2 * (attempt + 1)\n",
        "                print(f\"[github] {code} retrying in {delay:.1f}s \u2026\")\n",
        "                time.sleep(delay)\n",
        "                continue\n",
        "            raise\n",
        "    raise RuntimeError(f\"Failed to upload to GitHub after {max_retries} attempts.\")\n",
        "\n"
      ],
      "id": "P5aiRU9Z71ek"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArNeT6hO71ek"
      },
      "source": [
        "## 11. Cost Report Generation\n",
        "\n"
      ],
      "id": "ArNeT6hO71ek"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5gfo3mws71ek"
      },
      "outputs": [],
      "source": [
        "\n",
        "def generate_cost_report() -> str:\n",
        "    \"\"\"Generates a formatted Markdown string from the COST_LEDGER.\"\"\"\n",
        "    print(\"Generating cost report...\")\n",
        "    global COST_LEDGER, TOTAL_ESTIMATED_COST, COST_LIMIT_USD\n",
        "\n",
        "    persist_cost_ledger()\n",
        "\n",
        "    report_lines = [\n",
        "        \"# Agent Run Cost Report\",\n",
        "        f\"**Run completed:** {time.ctime()}\",\n",
        "        f\"**Cost Limit:** ${COST_LIMIT_USD:.2f}\",\n",
        "        f\"**Total Estimated Cost:** ${TOTAL_ESTIMATED_COST:.6f}\",\n",
        "        \"\",\n",
        "        \"| Task | Status | Prompt Tokens | Output Tokens | Cost (USD) | Cumulative (USD) | Details |\",\n",
        "        \"| :-- | :-- | --: | --: | --: | --: | :-- |\",\n",
        "    ]\n",
        "\n",
        "    if not COST_LEDGER:\n",
        "        report_lines.append(\"| (no tasks) | | 0 | 0 | $0.000000 | $0.000000 | |\")\n",
        "    else:\n",
        "        for item in COST_LEDGER:\n",
        "            report_lines.append(\n",
        "                f\"| {item.get('task', 'Unknown Task')} | {item.get('status', 'n/a')} | \"\n",
        "                f\"{item.get('prompt_tokens', 0)} | {item.get('output_tokens', 0)} | \"\n",
        "                f\"${item.get('cost', 0.0):.6f} | ${item.get('cumulative_cost', 0.0):.6f} | {item.get('details', '')} |\"\n",
        "            )\n",
        "\n",
        "    report_lines.append(\"\")\n",
        "    report_lines.append(\"---\")\n",
        "    report_lines.append(\"*End of Report*\")\n",
        "    return \"\n",
        "\".join(report_lines)\n",
        "\n"
      ],
      "id": "5gfo3mws71ek"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB4_pUoZ71ek"
      },
      "source": [
        "## 12. Main Execution\n",
        "\n"
      ],
      "id": "IB4_pUoZ71ek"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Irqj414Y71ek",
        "outputId": "6857ae27-8996-45dc-dea5-f9ea8cf2643a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Colby Agentic Crew - GenAI + GitHub Ops\n",
            "==================================================\n",
            "Cost Limit set to: $25.00\n",
            "GitHub Repo set to: jmbish04/colby-agentic-crew\n",
            "NOTE: Make sure 'GOOGLE_API_KEY' and 'GITHUB_TOKEN' are set in Colab secrets (View > Show secrets).\n",
            "[drive] Mounting\u2026\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[drive] Ready: /content/drive/MyDrive/colby_agentic_crew\n",
            "Initializing clients...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1520374863.py:26: DeprecationWarning: Argument login_or_token is deprecated, please use auth=github.Auth.Token(...) instead\n",
            "  gh = Github(GITHUB_TOKEN, per_page=50)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GitHub client authenticated.\n",
            "All clients initialized successfully.\n",
            "\n",
            "--- Starting Task: Generate wrangler.toml ---\n",
            "Generating text for prompt: '\n",
            "        Generate a complete 'wrangler.toml' file ...'\n",
            "Warning: Could not pre-count tokens: system_instruction parameter is not supported in Gemini API.. Proceeding with rough estimate.\n",
            "[CostTracker] Task 'Generate wrangler.toml' cost: $0.000632\n",
            "[CostTracker] New total estimated cost: $0.0006\n",
            "Text generation successful.\n",
            "Generated wrangler.toml:\n",
            " As an expert AI assistant specializing in Cloudflare Workers and agentic systems, here is the complete, production-ready `wrangler.toml` configuration for your project, `colby-agentic-crew`.\n",
            "\n",
            "This configuration includes all specified bindings, using placeholders for IDs that must be generated when you run `wrangler deploy` or `wrangler d1 create`/`wrangler kv namespace create`.\n",
            "\n",
            "## `wrangler.toml`\n",
            "\n",
            "```toml\n",
            "# ==============================================================================\n",
            "# CORE WORKER CONFIGURATION\n",
            "# ==============================================================================\n",
            "name = \"colby-agentic-crew\"\n",
            "main = \"src/index.ts\"\n",
            "compatibility_date = \"2024-05-15\"\n",
            "workers_dev = true\n",
            "\n",
            "# Assuming you are using TypeScript, ensure your build process handles it.\n",
            "# If using the standard 'npm create cloudflare@latest', this is usually handled.\n",
            "# build = { command = \"npm run build\" }\n",
            "\n",
            "\n",
            "# ==============================================================================\n",
            "# DURABLE OBJECTS (AGENT STATE MANAGEMENT)\n",
            "# Binding: AGENT_SUPERVISOR\n",
            "# Class: AgentSupervisor\n",
            "# ==============================================================================\n",
            "[[durable_objects.bindings]]\n",
            "name = \"AGENT_SUPERVISOR\"\n",
            "class_name = \"AgentSupervisor\"\n",
            "\n",
            "# Define the Durable Object migration configuration\n",
            "# This ensures the DO namespace is created and managed.\n",
            "[[migrations]]\n",
            "tag = \"v1\"\n",
            "new_classes = [\"AgentSupervisor\"]\n",
            "\n",
            "\n",
            "# ==============================================================================\n",
            "# KV NAMESPACE (CREW MEMORY & CONFIGURATION)\n",
            "# Binding: CREW_MEMORY_KV\n",
            "# ==============================================================================\n",
            "[[kv_namespaces]]\n",
            "binding = \"CREW_MEMORY_KV\"\n",
            "# IMPORTANT: Replace this placeholder ID after running:\n",
            "# wrangler kv namespace create CREW_MEMORY_KV\n",
            "id = \"CREW_MEMORY_KV_ID_PLACEHOLDER\"\n",
            "\n",
            "\n",
            "# ==============================================================================\n",
            "# D1 DATABASE (CREW STATE & LOGGING)\n",
            "# Binding: CREW_STATE_DB\n",
            "# ==============================================================================\n",
            "[[d1_databases]]\n",
            "binding = \"CREW_STATE_DB\"\n",
            "database_name = \"colby_crew_state\"\n",
            "# IMPORTANT: Replace this placeholder ID after running:\n",
            "# wrangler d1 create colby_crew_state\n",
            "database_id = \"CREW_STATE_DB_ID_PLACEHOLDER\"\n",
            "\n",
            "\n",
            "# ==============================================================================\n",
            "# R2 BUCKET (AGENT ARTIFACTS & LARGE FILE STORAGE)\n",
            "# Binding: AGENT_ARTIFACTS_R2\n",
            "# ==============================================================================\n",
            "[[r2_buckets]]\n",
            "binding = \"AGENT_ARTIFACTS_R2\"\n",
            "bucket_name = \"colby-agent-artifacts\"\n",
            "# Note: R2 buckets are globally unique by name, so no ID is required here.\n",
            "\n",
            "# ==============================================================================\n",
            "# ENVIRONMENT VARIABLES (Example: API Keys)\n",
            "# ==============================================================================\n",
            "[vars]\n",
            "# OPENAI_API_KEY = \"...\"\n",
            "# AGENT_LOG_LEVEL = \"info\"\n",
            "```\n",
            "\n",
            "### Next Steps (Required Setup)\n",
            "\n",
            "Before deploying, you must create the resources bound by ID:\n",
            "\n",
            "1.  **Create KV Namespace:**\n",
            "    ```bash\n",
            "    wrangler kv namespace create CREW_MEMORY_KV\n",
            "    # Copy the resulting ID into the `wrangler.toml` file.\n",
            "    ```\n",
            "\n",
            "2.  **Create D1 Database:**\n",
            "    ```bash\n",
            "    wrangler d1 create colby_crew_state\n",
            "    # Copy the resulting database_id into the `wrangler.toml` file.\n",
            "    ```\n",
            "\n",
            "3.  **Deploy the Worker:**\n",
            "    ```bash\n",
            "    wrangler deploy\n",
            "    ```\n",
            "\n",
            "--- Saving wrangler.toml to Google Drive ---\n",
            "[drive] Saved \u2192 /content/drive/MyDrive/colby_agentic_crew/wrangler_config_1761422638.toml\n",
            "\n",
            "--- Saving wrangler.toml to GitHub ---\n",
            "[github] PUT jmbish04/colby-agentic-crew:main/agent_generated_files/wrangler_config_1761422638.toml (attempt 1)\n",
            "\n",
            "--- Starting Task: Generate README.md ---\n",
            "Generating text for prompt: '\n",
            "        Generate a professional README.md for the...'\n",
            "Warning: Could not pre-count tokens: system_instruction parameter is not supported in Gemini API.. Proceeding with rough estimate.\n",
            "[CostTracker] Task 'Generate README.md' cost: $0.000907\n",
            "[CostTracker] New total estimated cost: $0.0015\n",
            "Text generation successful.\n",
            "Generated README.md:\n",
            " The following is a professional, production-ready `README.md` for your project, detailing the architecture and configuration using the specified Cloudflare services and agentic system concepts.\n",
            "\n",
            "---\n",
            "\n",
            "# `colby-agentic-crew`\n",
            "\n",
            "**Serverless Agentic Orchestration powered by Cloudflare and CrewAI**\n",
            "\n",
            "[![Cloudflare Workers](https://img.shields.io/badge/Cloudflare-Workers-orange?style=flat-square)](https://workers.cloudflare.com/)\n",
            "[![Durable Objects](https://img.shields.io/badge/State-Durable%20Objects-blueviolet?style=flat-square)](https://developers.cloudflare.com/durable-objects/)\n",
            "[![Database](https://img.shields.io/badge/Database-D1-lightgray?style=flat-square)](https://developers.cloudflare.com/d1/)\n",
            "[![Storage](https://img.shields.io/badge/Storage-R2-teal?style=flat-square)](https://developers.cloudflare.com/r2/)\n",
            "[![Configuration](https://img.shields.io/badge/Config-KV-yellow?style=flat-square)](https://developers.cloudflare.com/kv/)\n",
            "\n",
            "## \ud83d\udcdd Description\n",
            "\n",
            "The `colby-agentic-crew` project provides a highly scalable, stateful, and serverless infrastructure for running complex agentic workflows orchestrated by frameworks like CrewAI.\n",
            "\n",
            "By leveraging the Cloudflare ecosystem, this system ensures low-latency execution, persistent state management via Durable Objects, structured logging via D1, and robust artifact storage via R2, making it ideal for long-running, resource-intensive AI tasks.\n",
            "\n",
            "## \u2728 Features\n",
            "\n",
            "*   **Stateful Execution:** Durable Objects (DOs) manage the lifecycle and state of individual agent crews, ensuring persistence and fault tolerance throughout multi-step processes.\n",
            "*   **Scalability:** Cloudflare Workers handle API requests, automatically scaling to meet demand without requiring traditional server management.\n",
            "*   **Integrated Data Storage:**\n",
            "    *   **D1:** Used for structured data, logging crew history, agent performance metrics, and task definitions.\n",
            "    *   **R2:** Used for storing large outputs, generated reports, images, or datasets produced by the agent crews.\n",
            "*   **Dynamic Configuration:** Cloudflare KV provides fast, global access to configuration settings, API keys, and feature flags.\n",
            "*   **Unified Configuration:** All service bindings and environment variables are centrally managed within the `wrangler.toml` file.\n",
            "\n",
            "## \ud83c\udfd7\ufe0f Architecture Overview\n",
            "\n",
            "| Component | Cloudflare Service | Role in Agentic System |\n",
            "| :--- | :--- | :--- |\n",
            "| **API Gateway** | Cloudflare Worker | Receives requests (e.g., `POST /start_crew`), handles routing, authentication, and initial validation. |\n",
            "| **Crew Manager** | Durable Object (DO) | The stateful core. Each DO instance represents a single running CrewAI process, managing its internal state, communication, and task execution lifecycle. |\n",
            "| **Structured Data** | D1 Database | Stores metadata about crews, detailed execution logs, agent definitions, and user data. |\n",
            "| **Artifact Storage** | R2 Bucket | Stores final outputs, large files, and intermediate results generated by the agents. |\n",
            "| **Global Config** | KV Namespace | Stores runtime configuration, LLM API keys, system prompts, and feature toggles accessible by Workers and DOs. |\n",
            "\n",
            "## \u2699\ufe0f Prerequisites\n",
            "\n",
            "To deploy and run this project, you will need:\n",
            "\n",
            "1.  **Cloudflare Account:** Access to the Cloudflare dashboard and API tokens.\n",
            "2.  **Node.js & npm:** (LTS recommended) for running `wrangler`.\n",
            "3.  **`wrangler` CLI:** The Cloudflare command-line tool.\n",
            "    ```bash\n",
            "    npm install -g wrangler\n",
            "    ```\n",
            "4.  **Python Environment:** (If integrating the actual CrewAI logic) While the Worker/DO environment is JavaScript/TypeScript, the agent logic often runs in a subprocess or via an external service triggered by the DO. For local development or testing the agent logic, a Python environment is necessary.\n",
            "\n",
            "## \ud83d\ude80 Setup and Installation\n",
            "\n",
            "### 1. Clone the Repository\n",
            "\n",
            "```bash\n",
            "git clone https://github.com/your-org/colby-agentic-crew.git\n",
            "cd colby-agentic-crew\n",
            "```\n",
            "\n",
            "### 2. Install Dependencies\n",
            "\n",
            "```bash\n",
            "npm install\n",
            "```\n",
            "\n",
            "### 3. Authenticate Wrangler\n",
            "\n",
            "Log in to your Cloudflare account via the CLI:\n",
            "\n",
            "```bash\n",
            "wrangler login\n",
            "```\n",
            "\n",
            "### 4. Provision Resources\n",
            "\n",
            "Before deployment, you must create the necessary resources (D1 database, R2 bucket, and KV namespace) via the `wrangler` CLI or the Cloudflare dashboard.\n",
            "\n",
            "**Example Provisioning Commands:**\n",
            "\n",
            "```bash\n",
            "# Create D1 Database\n",
            "wrangler d1 create colby-crew-db\n",
            "\n",
            "# Create R2 Bucket\n",
            "wrangler r2 bucket create colby-crew-artifacts\n",
            "\n",
            "# Create KV Namespace\n",
            "wrangler kv namespace create colby_crew_config\n",
            "```\n",
            "\n",
            "## \ud83d\udee0\ufe0f Configuration (`wrangler.toml`)\n",
            "\n",
            "The `wrangler.toml` file is the central configuration hub, defining the Worker entry point, environment variables, and crucially, the bindings to all necessary Cloudflare services.\n",
            "\n",
            "Ensure your `wrangler.toml` is correctly configured with the IDs and names generated during the provisioning step.\n",
            "\n",
            "### Example `wrangler.toml` Structure\n",
            "\n",
            "```toml\n",
            "name = \"colby-agentic-crew\"\n",
            "main = \"src/index.ts\"\n",
            "compatibility_date = \"2024-01-01\"\n",
            "workers_dev = true\n",
            "\n",
            "[vars]\n",
            "# Example environment variable accessible in the Worker/DO\n",
            "LLM_MODEL = \"gpt-4o\"\n",
            "\n",
            "# ---\n",
            "\n",
            "--- Saving README.md to Google Drive ---\n",
            "[drive] Saved \u2192 /content/drive/MyDrive/colby_agentic_crew/README_1761422638.md\n",
            "\n",
            "--- Saving README.md to GitHub ---\n",
            "[github] PUT jmbish04/colby-agentic-crew:main/agent_generated_files/README_1761422638.md (attempt 1)\n",
            "\n",
            "--- 6. Performing GitHub Issue Operations ---\n",
            "Listing last 5 'open' issues for jmbish04/colby-agentic-crew...\n",
            "Found 0 issues.\n",
            "Existing open issues: []\n",
            "Creating issue in jmbish04/colby-agentic-crew: 'Bot Task: Review new `wrangler.toml` (1761422638)'\n",
            "Issue #1 created successfully.\n",
            "Commenting on issue #1 in jmbish04/colby-agentic-crew...\n",
            "Comment posted: https://github.com/jmbish04/colby-agentic-crew/issues/1#issuecomment-3447736045\n",
            "\n",
            "==================================================\n",
            "--- 7. Generating Final Cost Report ---\n",
            "Generating cost report...\n",
            "# Agent Run Cost Report\n",
            "**Run completed:** Sat Oct 25 20:04:27 2025\n",
            "**Cost Limit:** $25.00\n",
            "**Total Estimated Cost:** $0.001539\n",
            "\n",
            "## Itemized Costs\n",
            "\n",
            "| Task | Details (Tokens) | Cost (USD) |\n",
            "| :--- | :--- | :--- |\n",
            "| Main Execution (CRITICAL FAIL) | Models.generate_content() got an unexpected keyword argument 'system_instruction' | $0.000000 |\n",
            "| Main Execution (CRITICAL FAIL) | Models.generate_content() got an unexpected keyword argument 'system_instruction' | $0.000000 |\n",
            "| Generate wrangler.toml | 165 in, 820 out | $0.000632 |\n",
            "| Save wrangler.toml to Drive | File I/O | $0.000000 |\n",
            "| Save wrangler.toml to GitHub | GitHub API call | $0.000000 |\n",
            "| Generate README.md | 101 in, 1245 out | $0.000907 |\n",
            "| Save README.md to Drive | File I/O | $0.000000 |\n",
            "| Save README.md to GitHub | GitHub API call | $0.000000 |\n",
            "| List GitHub Issues | GitHub API call | $0.000000 |\n",
            "| Create GitHub Issue (wrangler.toml) | Created #1 | $0.000000 |\n",
            "| Comment on GitHub Issue | Commented on #1 | $0.000000 |\n",
            "\n",
            "---\n",
            "*End of Report*\n",
            "\n",
            "--- 8. Saving Final Cost Report ---\n",
            "[drive] Saved \u2192 /content/drive/MyDrive/colby_agentic_crew/cost_report_1761422638.md\n",
            "[github] PUT jmbish04/colby-agentic-crew:main/agent_generated_files/cost_report_1761422638.md (attempt 1)\n",
            "\n",
            "==================================================\n",
            "Script finished.\n",
            "FINAL ESTIMATED COST: $0.001539\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 50)\n",
        "print(\"Colby Agentic Crew - GenAI + GitHub Ops\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Cost Limit set to: ${COST_LIMIT_USD:.2f}\")\n",
        "print(f\"GitHub Repo set to: {GITHUB_REPO}\")\n",
        "print(f\"Dry run mode: {DRY_RUN}\")\n",
        "print(\"NOTE: Make sure 'GOOGLE_API_KEY' and 'GITHUB_TOKEN' are set in Colab secrets (View > Show secrets).\")\n",
        "ensure_directory(OUTPUT_ROOT_PATH)\n",
        "drive_ready = ensure_drive_ready()\n",
        "cloudflare_assets: Dict[str, Any] = {}\n",
        "docs_reference: Dict[str, Any] = {}\n",
        "generated_text_toml: Optional[str] = None\n",
        "generated_text_readme: Optional[str] = None\n",
        "generated_text_agents: Optional[str] = None\n",
        "now = datetime.utcnow()\n",
        "timestamp_label = now.strftime(\"%Y%m%d-%H%M%S\")\n",
        "try:\n",
        "    if not drive_ready and COLAB_ENV:\n",
        "        print(\"Could not mount Google Drive. Aborting to prevent lost work.\")\n",
        "        sys.exit(1)\n",
        "    clients = init_clients()\n",
        "    if clients:\n",
        "        if DRY_RUN:\n",
        "            print(\"[dry-run] Gemini generation will be replaced with placeholder text.\")\n",
        "        print(\"\n",
        "--- 0. Discovering Cloudflare repositories ---\")\n",
        "        cloudflare_assets = prepare_cloudflare_assets(REPO_KEYWORDS, dry_run=DRY_RUN)\n",
        "        summary_payload = cloudflare_assets.get(\"summary\", {})\n",
        "        code_index_payload = cloudflare_assets.get(\"code_index\", {})\n",
        "        repo_context = cloudflare_assets.get(\"context\", \"\")\n",
        "        summary_json = json.dumps(summary_payload, indent=2)\n",
        "        index_json = json.dumps(code_index_payload, indent=2)\n",
        "        save_to_drive(SUMMARY_FILENAME, summary_json)\n",
        "        save_to_drive(CODE_INDEX_FILENAME, index_json)\n",
        "        if not DRY_RUN:\n",
        "            try:\n",
        "                save_to_github(\n",
        "                    path=f\"{GITHUB_SAVE_PATH}/{SUMMARY_FILENAME}\",\n",
        "                    content=summary_json,\n",
        "                    commit_message=f\"Agent: Update {SUMMARY_FILENAME} ({timestamp_label})\"\n",
        "                )\n",
        "                save_to_github(\n",
        "                    path=f\"{GITHUB_SAVE_PATH}/{CODE_INDEX_FILENAME}\",\n",
        "                    content=index_json,\n",
        "                    commit_message=f\"Agent: Update {CODE_INDEX_FILENAME} ({timestamp_label})\"\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: unable to push discovery artifacts to GitHub: {e}\")\n",
        "        else:\n",
        "            print(\"[dry-run] Skipping GitHub upload for discovery artifacts.\")\n",
        "        summary_context = truncate_for_context(summary_json, limit=CONTEXT_MAX_CHARS // 2)\n",
        "        index_context = truncate_for_context(index_json, limit=CONTEXT_MAX_CHARS // 2)\n",
        "        combined_context = truncate_for_context(\n",
        "            f\"Cloudflare Repository Summary (JSON):\n",
        "{summary_context}\n",
        "\"\n",
        "            f\"Code Index Snapshot (JSON):\n",
        "{index_context}\n",
        "\"\n",
        "            f\"Narrative Context:\n",
        "{repo_context}\",\n",
        "            limit=CONTEXT_MAX_CHARS,\n",
        "        )\n",
        "        if DRY_RUN:\n",
        "            generated_text_toml = \"# Dry run - wrangler.toml not generated.\"\n",
        "        else:\n",
        "            print(\"\n",
        "--- 1. Generating wrangler.toml ---\")\n",
        "            wrangler_prompt = \"\"\"\n",
        "            Using the supplied Cloudflare repository context, draft a production-ready `wrangler.toml`\n",
        "            for the `colby-agentic-crew` project. The configuration must:\n",
        "            - Define a Workers script entry at `src/index.ts`.\n",
        "            - Register Durable Object bindings named `AGENT_SUPERVISOR` (class `AgentSupervisor`) and `TASK_LOCKER` (class `TaskLocker`).\n",
        "            - Include KV (`CREW_MEMORY_KV`), D1 (`CREW_STATE_DB`), R2 (`AGENT_ARTIFACTS_R2`), and Queues (`WORKER_QUEUE`) bindings.\n",
        "            - Demonstrate usage of Workers AI and Workflows where applicable.\n",
        "            - Reference any relevant Cloudflare services discovered in the source repos.\n",
        "            Provide full TOML including environments for development and production.\n",
        "            \"\"\"\n",
        "            generated_text_toml = gen_text(\n",
        "                clients,\n",
        "                task_name=\"Generate wrangler.toml\",\n",
        "                prompt=wrangler_prompt,\n",
        "                additional_context=combined_context,\n",
        "            )\n",
        "        if generated_text_toml:\n",
        "            print(\"Generated wrangler.toml:\n",
        "\", generated_text_toml)\n",
        "            output_filename_toml = f\"wrangler_config_{timestamp_label}.toml\"\n",
        "            save_to_drive(output_filename_toml, generated_text_toml)\n",
        "            save_to_drive(f\"{CREW_TEMPLATE_DIR_NAME}/wrangler.toml\", generated_text_toml)\n",
        "            if not DRY_RUN:\n",
        "                try:\n",
        "                    save_to_github(\n",
        "                        path=f\"{GITHUB_SAVE_PATH}/{output_filename_toml}\",\n",
        "                        content=generated_text_toml,\n",
        "                        commit_message=f\"Agent: Add wrangler.toml config ({timestamp_label})\"\n",
        "                    )\n",
        "                    save_to_github(\n",
        "                        path=f\"{GITHUB_SAVE_PATH}/{CREW_TEMPLATE_DIR_NAME}/wrangler.toml\",\n",
        "                        content=generated_text_toml,\n",
        "                        commit_message=f\"Agent: Sync template wrangler.toml ({timestamp_label})\"\n",
        "                    )\n",
        "                except Exception as e:\n",
        "                    print(f\"Error saving wrangler.toml to GitHub: {e}\")\n",
        "            else:\n",
        "                print(\"[dry-run] Skipping GitHub upload for wrangler.toml.\")\n",
        "        else:\n",
        "            print(\"Skipping wrangler.toml saving steps as generation failed or was cost-blocked.\")\n",
        "        if DRY_RUN:\n",
        "            generated_text_readme = \"# Dry run - README not generated.\"\n",
        "        else:\n",
        "            print(\"\n",
        "--- 2. Generating README.md ---\")\n",
        "            readme_prompt = \"\"\"\n",
        "            Create a comprehensive README for the Colby Agentic Crew template. The README must:\n",
        "            - Summarize architecture patterns observed in the Cloudflare repositories.\n",
        "            - Document how Durable Objects, Queues, KV, R2, and D1 are wired together.\n",
        "            - Include deployment steps with Wrangler, Workers AI integration notes, and token budget guidance.\n",
        "            - Reference the generated wrangler.toml and highlight how to extend the template.\n",
        "            \"\"\"\n",
        "            generated_text_readme = gen_text(\n",
        "                clients,\n",
        "                task_name=\"Generate README.md\",\n",
        "                prompt=readme_prompt,\n",
        "                additional_context=combined_context,\n",
        "            )\n",
        "        if generated_text_readme:\n",
        "            print(\"Generated README.md:\n",
        "\", generated_text_readme)\n",
        "            output_filename_readme = f\"README_{timestamp_label}.md\"\n",
        "            save_to_drive(output_filename_readme, generated_text_readme)\n",
        "            save_to_drive(f\"{CREW_TEMPLATE_DIR_NAME}/README.md\", generated_text_readme)\n",
        "            if not DRY_RUN:\n",
        "                try:\n",
        "                    save_to_github(\n",
        "                        path=f\"{GITHUB_SAVE_PATH}/{output_filename_readme}\",\n",
        "                        content=generated_text_readme,\n",
        "                        commit_message=f\"Agent: Add README.md documentation ({timestamp_label})\"\n",
        "                    )\n",
        "                    save_to_github(\n",
        "                        path=f\"{GITHUB_SAVE_PATH}/{CREW_TEMPLATE_DIR_NAME}/README.md\",\n",
        "                        content=generated_text_readme,\n",
        "                        commit_message=f\"Agent: Sync template README ({timestamp_label})\"\n",
        "                    )\n",
        "                except Exception as e:\n",
        "                    print(f\"Error saving README to GitHub: {e}\")\n",
        "            else:\n",
        "                print(\"[dry-run] Skipping GitHub upload for README.\")\n",
        "        else:\n",
        "            print(\"Skipping README.md saving steps as generation failed or was cost-blocked.\")\n",
        "        print(\"\n",
        "--- 3. Fetching Cloudflare documentation references ---\")\n",
        "        try:\n",
        "            docs_reference = clients.docs_client.gather_docs(DOC_TOPICS_ORDER)\n",
        "            docs_summary_text = clients.docs_client.summarize_for_prompt(docs_reference, limit=4000)\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: unable to gather docs: {e}\")\n",
        "            docs_reference = {\n",
        "                \"queried_at\": timestamp_iso(),\n",
        "                \"topics\": DOC_TOPICS_ORDER,\n",
        "                \"documents\": [],\n",
        "                \"error\": str(e),\n",
        "            }\n",
        "            docs_summary_text = \"Documentation retrieval failed; proceed with repository context only.\"\n",
        "        docs_json = json.dumps(docs_reference, indent=2)\n",
        "        save_to_drive(DOCS_REFERENCE_FILENAME, docs_json)\n",
        "        if not DRY_RUN:\n",
        "            try:\n",
        "                save_to_github(\n",
        "                    path=f\"{GITHUB_SAVE_PATH}/{DOCS_REFERENCE_FILENAME}\",\n",
        "                    content=docs_json,\n",
        "                    commit_message=f\"Agent: Update {DOCS_REFERENCE_FILENAME} ({timestamp_label})\"\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: unable to push docs reference to GitHub: {e}\")\n",
        "        else:\n",
        "            print(\"[dry-run] Skipping GitHub upload for docs reference.\")\n",
        "        agents_context = truncate_for_context(\n",
        "            f\"{combined_context}\n",
        "Cloudflare Docs Excerpts:\n",
        "{docs_summary_text}\",\n",
        "            limit=CONTEXT_MAX_CHARS,\n",
        "        )\n",
        "        if DRY_RUN:\n",
        "            generated_text_agents = \"# Dry run - AGENTS instructions not generated.\"\n",
        "        else:\n",
        "            print(\"\n",
        "--- 4. Generating AGENTS.md ---\")\n",
        "            agents_prompt = \"\"\"\n",
        "            Produce an `AGENTS.md` file describing how a Crew of AI agents should collaborate on this project.\n",
        "            Include:\n",
        "            - Roles for planner, researcher, implementer, and reviewer agents.\n",
        "            - How to leverage the downloaded Cloudflare repositories for prior art.\n",
        "            - Steps for updating bindings, Wrangler configs, and deployment pipelines.\n",
        "            - Guidance on cost controls and when to consult the fetched Cloudflare docs.\n",
        "            \"\"\"\n",
        "            generated_text_agents = gen_text(\n",
        "                clients,\n",
        "                task_name=\"Generate AGENTS.md\",\n",
        "                prompt=agents_prompt,\n",
        "                additional_context=agents_context,\n",
        "            )\n",
        "        if generated_text_agents:\n",
        "            print(\"Generated AGENTS.md:\n",
        "\", generated_text_agents)\n",
        "            output_filename_agents = f\"AGENTS_{timestamp_label}.md\"\n",
        "            save_to_drive(output_filename_agents, generated_text_agents)\n",
        "            save_to_drive(f\"{CREW_TEMPLATE_DIR_NAME}/AGENTS.md\", generated_text_agents)\n",
        "            if not DRY_RUN:\n",
        "                try:\n",
        "                    save_to_github(\n",
        "                        path=f\"{GITHUB_SAVE_PATH}/{output_filename_agents}\",\n",
        "                        content=generated_text_agents,\n",
        "                        commit_message=f\"Agent: Add AGENTS.md instructions ({timestamp_label})\"\n",
        "                    )\n",
        "                    save_to_github(\n",
        "                        path=f\"{GITHUB_SAVE_PATH}/{CREW_TEMPLATE_DIR_NAME}/AGENTS.md\",\n",
        "                        content=generated_text_agents,\n",
        "                        commit_message=f\"Agent: Sync template AGENTS ({timestamp_label})\"\n",
        "                    )\n",
        "                except Exception as e:\n",
        "                    print(f\"Error saving AGENTS.md to GitHub: {e}\")\n",
        "            else:\n",
        "                print(\"[dry-run] Skipping GitHub upload for AGENTS.\")\n",
        "        else:\n",
        "            print(\"Skipping AGENTS.md saving steps as generation failed or was cost-blocked.\")\n",
        "        print(\"\n",
        "--- 5. Building Crew template workspace ---\")\n",
        "        template_path = ensure_directory(OUTPUT_ROOT_PATH / CREW_TEMPLATE_DIR_NAME)\n",
        "        if summary_json:\n",
        "            (template_path / SUMMARY_FILENAME).write_text(summary_json, encoding=\"utf-8\")\n",
        "        if index_json:\n",
        "            (template_path / CODE_INDEX_FILENAME).write_text(index_json, encoding=\"utf-8\")\n",
        "        if docs_reference:\n",
        "            (template_path / DOCS_REFERENCE_FILENAME).write_text(docs_json, encoding=\"utf-8\")\n",
        "        print(f\"Template assets stored at {template_path}\")\n",
        "        print(\"\n",
        "--- 6. Performing GitHub Issue Operations ---\")\n",
        "        if DRY_RUN:\n",
        "            print(\"[dry-run] Skipping GitHub issue automation.\")\n",
        "        elif GITHUB_REPO == \"jmbish04/colby-agentic-crew\":\n",
        "            try:\n",
        "                issues = gh_list_issues(clients.gh, GITHUB_REPO, state=\"open\", limit=5)\n",
        "                print(\"Existing open issues:\", json.dumps(issues, indent=2))\n",
        "                if generated_text_toml:\n",
        "                    issue_title = f\"Bot Task: Review new wrangler.toml ({timestamp_label})\"\n",
        "                    issue_body = (\n",
        "                        \"Generated by agent.\n",
        "\"\n",
        "                        f\"See Drive output: `{output_filename_toml}`\n",
        "\"\n",
        "                        \"**Configuration Preview:**\n",
        "\"\n",
        "                        f\"```toml\n",
        "{generated_text_toml[:2000]}\n",
        "```\n",
        "\"\n",
        "                    )\n",
        "                    new_issue_num = gh_create_issue(\n",
        "                        clients.gh,\n",
        "                        GITHUB_REPO,\n",
        "                        issue_title,\n",
        "                        issue_body,\n",
        "                        labels=[\"bot\", \"config\"],\n",
        "                    )\n",
        "                    comment_body = \"Task created automatically.\"\n",
        "                    if generated_text_readme:\n",
        "                        comment_body += f\"\n",
        "README draft: `{output_filename_readme}`\"\n",
        "                    gh_comment_issue(clients.gh, GITHUB_REPO, new_issue_num, comment_body)\n",
        "            except GithubException as e:\n",
        "                print(f\"Error during GitHub operations: {e}\")\n",
        "                print(f\"Please ensure the token has 'repo' scope and access to '{GITHUB_REPO}'.\")\n",
        "            except Exception as e:\n",
        "                print(f\"An unexpected error occurred during GitHub ops: {e}\")\n",
        "    else:\n",
        "        print(\"Failed to initialize clients. Check secrets.\")\n",
        "except Exception as e:\n",
        "    print(f\"An uncaught error occurred in main execution: {e}\")\n",
        "finally:\n",
        "    print(\"\n",
        "\" + \"=\" * 50)\n",
        "    print(\"--- 7. Generating Final Cost Report ---\")\n",
        "    report_content = generate_cost_report()\n",
        "    print(report_content)\n",
        "    print(\"\n",
        "--- 8. Saving Final Cost Report ---\")\n",
        "    report_filename = f\"cost_report_{timestamp_label}.md\"\n",
        "    save_to_drive(report_filename, report_content)\n",
        "    if not DRY_RUN:\n",
        "        try:\n",
        "            save_to_github(\n",
        "                path=f\"{GITHUB_SAVE_PATH}/{report_filename}\",\n",
        "                content=report_content,\n",
        "                commit_message=f\"Agent: Final cost report ({timestamp_label})\"\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving final cost report to GitHub: {e}\")\n",
        "    else:\n",
        "        print(\"[dry-run] Skipping GitHub upload for cost report.\")\n",
        "    print(\"\n",
        "\" + \"=\" * 50)\n",
        "    print(\"Script finished.\")\n",
        "    print(f\"FINAL ESTIMATED COST: ${TOTAL_ESTIMATED_COST:.6f}\")\n",
        "    if COST_LOG_PATH:\n",
        "        print(f\"Cost ledger: {COST_LOG_PATH}\")\n",
        "    print(\"=\" * 50)\n"
      ],
      "id": "Irqj414Y71ek"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}