{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6cbbQjG71eh"
      },
      "source": [
        "# Colby Agentic Crew - GenAI + GitHub Ops\n",
        "\n",
        "This notebook contains the complete agent script. Run the cells in order.\n",
        "\n",
        "**First, run this cell to install the required Python libraries.**"
      ],
      "id": "P6cbbQjG71eh"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdtL9Fqr71ei",
        "outputId": "5cb7b51d-f9ba-4de5-9051-42f522442b22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m432.7/432.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# In a Colab cell, run this first to install dependencies:\n",
        "!pip -q install google-genai PyGithub requests"
      ],
      "id": "fdtL9Fqr71ei"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yV-MroW71ei"
      },
      "source": [
        "## 1. Imports"
      ],
      "id": "2yV-MroW71ei"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PE6PXwdp71ei"
      },
      "outputs": [],
      "source": [
        "# @title  imports\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import base64\n",
        "import requests\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, List, Tuple\n",
        "from io import BytesIO\n",
        "\n",
        "# --- Google/Colab Modules ---\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    from google.colab import userdata\n",
        "    COLAB_ENV = True\n",
        "except ImportError:\n",
        "    COLAB_ENV = False\n",
        "\n",
        "# --- Google GenAI SDK ---\n",
        "try:\n",
        "    from google import genai\n",
        "    from google.genai import types\n",
        "except ImportError:\n",
        "    print(\"Error: 'google-genai' library not found. Please run: !pip install google-genai\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# --- GitHub SDK ---\n",
        "try:\n",
        "    from github import Github, GithubException\n",
        "except ImportError:\n",
        "    print(\"Error: 'PyGithub' library not found. Please run: !pip install PyGithub\")\n",
        "    sys.exit(1)\n"
      ],
      "id": "PE6PXwdp71ei"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWLk-Zkf71ei"
      },
      "source": [
        "## 2. Configuration\n",
        "\n",
        "Set cost limits, API models, and file paths here."
      ],
      "id": "KWLk-Zkf71ei"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TWSHF-pl71ei"
      },
      "outputs": [],
      "source": [
        "# --- Model Configuration ---\n",
        "# Using latest preview models as of Oct 2025\n",
        "GEN_MODEL_TEXT = \"gemini-2.5-flash-preview-09-2025\"\n",
        "# (Image model removed)\n",
        "\n",
        "# --- Cost Control Configuration ---\n",
        "# Prices per 1,000,000 tokens (as of late 2025 previews)\n",
        "# $0.35 / 1M input tokens, $0.70 / 1M output tokens for flash\n",
        "PRICE_FLASH_INPUT_PER_TOKEN = 0.35 / 1_000_000\n",
        "PRICE_FLASH_OUTPUT_PER_TOKEN = 0.70 / 1_000_000\n",
        "\n",
        "# (Image cost removed)\n",
        "\n",
        "COST_LIMIT_USD = 25.00\n",
        "TOTAL_ESTIMATED_COST = 0.0  # Global cost tracker\n",
        "COST_LEDGER = [] # New: Itemized list of costs\n",
        "\n",
        "\n",
        "# --- Google Drive Configuration ---\n",
        "GDRIVE_MOUNT_POINT = '/content/drive'\n",
        "OUTPUT_FOLDER_NAME = 'colby_agentic_crew'\n",
        "OUTPUT_FOLDER_PATH = os.path.join(GDRIVE_MOUNT_POINT, 'MyDrive', OUTPUT_FOLDER_NAME)\n",
        "\n",
        "# --- GitHub Configuration ---\n",
        "# Updated to your specified repo\n",
        "GITHUB_REPO = \"jmbish04/colby-agentic-crew\"\n",
        "GITHUB_SAVE_PATH = \"agent_generated_files\" # Subfolder in the repo\n",
        "GITHUB_TOKEN = \"\" # Will be loaded by init_clients"
      ],
      "id": "TWSHF-pl71ei"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMikfwMg71ej"
      },
      "source": [
        "## 3. Utilities\n",
        "\n",
        "Helper functions for reading secrets and API call backoff."
      ],
      "id": "gMikfwMg71ej"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cw72CVsk71ej"
      },
      "outputs": [],
      "source": [
        "def read_secret(name: str, prompt: Optional[str] = None) -> str:\n",
        "    \"\"\"\n",
        "    Read a secret from Colab userdata if available, else environment, else prompt stdin.\n",
        "    \"\"\"\n",
        "    # 1) Colab userdata (preferred)\n",
        "    if COLAB_ENV:\n",
        "        try:\n",
        "            val = userdata.get(name) # Raises if not set\n",
        "            if val:\n",
        "                return val\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # 2) Environment variable\n",
        "    val = os.getenv(name)\n",
        "    if val:\n",
        "        return val\n",
        "\n",
        "    # 3) Stdin prompt (last resort, for local testing)\n",
        "    if 'getpass' not in sys.modules:\n",
        "        import getpass\n",
        "\n",
        "    if prompt is None:\n",
        "        prompt = f\"Enter value for {name}: \"\n",
        "    try:\n",
        "        return getpass.getpass(prompt)\n",
        "    except Exception:\n",
        "        # If getpass fails (non-tty), fall back to input()\n",
        "        return input(prompt)\n",
        "\n",
        "\n",
        "def backoff_sleep(retry: int):\n",
        "    # simple exponential backoff with jitter\n",
        "    delay = min(2 ** retry, 30) + (0.1 * retry)\n",
        "    print(f\"  ...backing off for {delay:.1f}s\")\n",
        "    time.sleep(delay)"
      ],
      "id": "cw72CVsk71ej"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SA-mh3wC71ej"
      },
      "source": [
        "## 4. Cost Control\n",
        "\n",
        "Functions to track and enforce the `$25.00` spending limit."
      ],
      "id": "SA-mh3wC71ej"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9okq3gKK71ej"
      },
      "outputs": [],
      "source": [
        "def check_and_update_cost(\n",
        "    task_name: str,\n",
        "    prompt_tokens: int = 0,\n",
        "    output_tokens: int = 0,\n",
        "    # images_generated (removed)\n",
        "    model: str = GEN_MODEL_TEXT\n",
        ") -> bool:\n",
        "    \"\"\"\n",
        "    Checks if a new call would exceed the limit, logs the itemized cost,\n",
        "    and updates the global cost.\n",
        "    Returns True if the call can proceed, False if it's blocked.\n",
        "    \"\"\"\n",
        "    global TOTAL_ESTIMATED_COST, COST_LEDGER\n",
        "\n",
        "    call_cost = 0.0\n",
        "    if model == GEN_MODEL_TEXT:\n",
        "        call_cost += prompt_tokens * PRICE_FLASH_INPUT_PER_TOKEN\n",
        "        call_cost += output_tokens * PRICE_FLASH_OUTPUT_PER_TOKEN\n",
        "    # (Image cost logic removed)\n",
        "\n",
        "    if (TOTAL_ESTIMATED_COST + call_cost) > COST_LIMIT_USD:\n",
        "        print(\"=\"*50)\n",
        "        print(f\"COST LIMIT BREACHED: Task '{task_name}' cannot proceed.\")\n",
        "        print(f\"  Current Cost: ${TOTAL_ESTIMATED_COST:.4f}\")\n",
        "        print(f\"  Attempted Call Cost: ${call_cost:.4f}\")\n",
        "        print(f\"  Limit: ${COST_LIMIT_USD:.4f}\")\n",
        "        print(\"=\"*50)\n",
        "        # Log the breach attempt\n",
        "        COST_LEDGER.append({\n",
        "            \"task\": f\"FAILED: {task_name} (Cost Limit Breach)\",\n",
        "            \"cost\": call_cost,\n",
        "            \"details\": f\"Attempted {prompt_tokens} in, {output_tokens} out\"\n",
        "        })\n",
        "        return False\n",
        "\n",
        "    # --- Log and Update ---\n",
        "    TOTAL_ESTIMATED_COST += call_cost\n",
        "    item_details = f\"{prompt_tokens} in, {output_tokens} out\"\n",
        "    COST_LEDGER.append({\n",
        "        \"task\": task_name,\n",
        "        \"cost\": call_cost,\n",
        "        \"details\": item_details\n",
        "    })\n",
        "    print(f\"[CostTracker] Task '{task_name}' cost: ${call_cost:.6f}\")\n",
        "    print(f\"[CostTracker] New total estimated cost: ${TOTAL_ESTIMATED_COST:.4f}\")\n",
        "    return True"
      ],
      "id": "9okq3gKK71ej"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfwBjZH771ej"
      },
      "source": [
        "## 5. API Clients\n",
        "\n",
        "Initialization for GenAI and GitHub clients."
      ],
      "id": "LfwBjZH771ej"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GGAlV5v_71ej"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Clients:\n",
        "    genai_client: genai.Client\n",
        "    gh: Github\n",
        "\n",
        "\n",
        "def init_clients() -> Optional[Clients]:\n",
        "    \"\"\"Initializes and returns all API clients.\"\"\"\n",
        "    global GITHUB_TOKEN # Load the global token\n",
        "    try:\n",
        "        print(\"Initializing clients...\")\n",
        "        # *** CHANGED as requested: Using GOOGLE_API_KEY ***\n",
        "        gemini_key = read_secret(\"GOOGLE_API_KEY\", \"GOOGLE_API_KEY (Google API key): \")\n",
        "        GITHUB_TOKEN = read_secret(\"GITHUB_TOKEN\", \"GITHUB_TOKEN (GitHub PAT): \")\n",
        "\n",
        "        if not gemini_key:\n",
        "            # *** CHANGED as requested ***\n",
        "            print(\"ERROR: GOOGLE_API_KEY is not set in Colab secrets (View > Show secrets).\")\n",
        "            return None\n",
        "\n",
        "        if not GITHUB_TOKEN:\n",
        "            print(\"ERROR: GITHUB_TOKEN is not set in Colab secrets (View > Show secrets).\")\n",
        "            return None\n",
        "\n",
        "        genai_client = genai.Client(api_key=gemini_key)\n",
        "        gh = Github(GITHUB_TOKEN, per_page=50)\n",
        "\n",
        "        # Test GitHub connection\n",
        "        _ = gh.get_user().login\n",
        "        print(\"GitHub client authenticated.\")\n",
        "\n",
        "        print(\"All clients initialized successfully.\")\n",
        "        return Clients(genai_client, gh)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing clients: {e}\")\n",
        "        if 'Bad credentials' in str(e):\n",
        "            print(\"Hint: Check your GITHUB_TOKEN.\")\n",
        "        return None"
      ],
      "id": "GGAlV5v_71ej"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2LLMKu771ej"
      },
      "source": [
        "## 6. Google GenAI Operations\n",
        "\n",
        "Core function for generating text."
      ],
      "id": "L2LLMKu771ej"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "j8NjNNrY71ej"
      },
      "outputs": [],
      "source": [
        "def gen_text(clients: Clients, task_name: str, prompt: str, max_retries: int = 3) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Generate text content using google.genai (Gemini Developer API via API key).\n",
        "    Uses config.system_instruction (no Vertex). Tracks cost with usage_metadata.\n",
        "    \"\"\"\n",
        "    global TOTAL_ESTIMATED_COST\n",
        "\n",
        "    SYSTEM_TEXT = (\n",
        "        \"You are an expert AI assistant specializing in Cloudflare Workers, \"\n",
        "        \"agentic systems, and CrewAI. Provide complete, production-ready code and configurations.\"\n",
        "    )\n",
        "\n",
        "    print(f\"\\n--- Starting Task: {task_name} ---\")\n",
        "    print(f\"Generating text for prompt: '{prompt[:50]}...'\")\n",
        "\n",
        "    # Build the exact inputs we’ll send for generation\n",
        "    contents = prompt  # SDK will wrap a string as a user message automatically\n",
        "\n",
        "    # --- Pre-call token count (use SAME inputs and config) ---\n",
        "    try:\n",
        "        ct = clients.genai_client.models.count_tokens(\n",
        "            model=GEN_MODEL_TEXT,\n",
        "            contents=contents,\n",
        "            config=types.GenerateContentConfig(\n",
        "                system_instruction=SYSTEM_TEXT,\n",
        "            ),\n",
        "        )\n",
        "        prompt_tokens = getattr(ct, \"total_tokens\", None) or 0\n",
        "        print(f\"[CostTracker] Estimated prompt tokens: {prompt_tokens}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not pre-count tokens: {e}. Proceeding with rough estimate.\")\n",
        "        prompt_tokens = max(1, (len(SYSTEM_TEXT) + len(prompt)) // 4)\n",
        "\n",
        "    pre_check_cost = prompt_tokens * PRICE_FLASH_INPUT_PER_TOKEN\n",
        "    if (TOTAL_ESTIMATED_COST + pre_check_cost) > COST_LIMIT_USD:\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"COST LIMIT BREACH: Pre-check for task '{task_name}' failed.\")\n",
        "        print(f\"  Current Cost: ${TOTAL_ESTIMATED_COST:.4f}\")\n",
        "        print(f\"  Est. Prompt Cost: ${pre_check_cost:.4f}\")\n",
        "        print(f\"  Limit: ${COST_LIMIT_USD:.4f}\")\n",
        "        print(\"=\" * 50)\n",
        "        COST_LEDGER.append({\n",
        "            \"task\": f\"SKIPPED: {task_name} (Pre-check failed cost limit)\",\n",
        "            \"cost\": 0.0,\n",
        "            \"details\": f\"Est. prompt tokens: {prompt_tokens}\"\n",
        "        })\n",
        "        return None\n",
        "\n",
        "    # --- API call with retries ---\n",
        "    for attempt in range(max_retries + 1):\n",
        "        try:\n",
        "            resp = clients.genai_client.models.generate_content(\n",
        "                model=GEN_MODEL_TEXT,\n",
        "                contents=contents,\n",
        "                config=types.GenerateContentConfig(\n",
        "                    system_instruction=SYSTEM_TEXT,\n",
        "                    temperature=0.3,\n",
        "                    top_p=0.9,\n",
        "                    max_output_tokens=2048,\n",
        "                    # If you want strict JSON outputs later:\n",
        "                    # response_mime_type=\"application/json\",\n",
        "                    # response_schema=...,\n",
        "                ),\n",
        "            )\n",
        "\n",
        "            # Cost tracking\n",
        "            usage = getattr(resp, \"usage_metadata\", None)\n",
        "            if usage:\n",
        "                in_tokens = getattr(usage, \"prompt_token_count\", 0)\n",
        "                out_tokens = getattr(usage, \"candidates_token_count\", 0)\n",
        "                ok = check_and_update_cost(\n",
        "                    task_name=task_name,\n",
        "                    prompt_tokens=in_tokens,\n",
        "                    output_tokens=out_tokens,\n",
        "                    model=GEN_MODEL_TEXT\n",
        "                )\n",
        "                if not ok:\n",
        "                    return \"(Cost limit reached during generation)\"\n",
        "            else:\n",
        "                print(\"Warning: usage_metadata missing; using pre-check only.\")\n",
        "                check_and_update_cost(\n",
        "                    task_name=f\"{task_name} (Usage data missing)\",\n",
        "                    prompt_tokens=prompt_tokens,\n",
        "                    output_tokens=0,\n",
        "                    model=GEN_MODEL_TEXT\n",
        "                )\n",
        "\n",
        "            # Collect text\n",
        "            text_out = getattr(resp, \"text\", \"\") or \"\"\n",
        "            if not text_out:\n",
        "                # Fallback: concatenate candidate parts if text is empty\n",
        "                parts = []\n",
        "                for cand in getattr(resp, \"candidates\", []) or []:\n",
        "                    for part in getattr(cand.content, \"parts\", []) or []:\n",
        "                        if getattr(part, \"text\", None):\n",
        "                            parts.append(part.text)\n",
        "                text_out = \"\\n\".join(parts).strip()\n",
        "\n",
        "            if not text_out:\n",
        "                raise RuntimeError(\"No text output received from model.\")\n",
        "\n",
        "            print(\"Text generation successful.\")\n",
        "            return text_out\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"GenAI text error (Attempt {attempt+1}): {e}\")\n",
        "            if attempt >= max_retries:\n",
        "                raise\n",
        "            backoff_sleep(attempt)\n",
        "\n",
        "    return None"
      ],
      "id": "j8NjNNrY71ej"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUOZhdtA71ek"
      },
      "source": [
        "## 7. GitHub Operations (SDK)\n",
        "\n",
        "Functions for managing GitHub Issues using the `PyGithub` SDK."
      ],
      "id": "gUOZhdtA71ek"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "fBKSXkU-71ek"
      },
      "outputs": [],
      "source": [
        "from typing import List, Optional, cast\n",
        "from github import Github\n",
        "from github.GithubException import GithubException\n",
        "from github.Repository import Repository\n",
        "from github.Issue import Issue\n",
        "from github.PaginatedList import PaginatedList\n",
        "\n",
        "def gh_get_repo(gh: Github, full_name: str) -> Repository:\n",
        "    \"\"\"Get a repo like 'owner/name'. Raises if not found or no access.\"\"\"\n",
        "    try:\n",
        "        repo = gh.get_repo(full_name)\n",
        "        return cast(Repository, repo)\n",
        "    except GithubException as e:\n",
        "        print(f\"GitHub repo error for {full_name}: {getattr(e, 'data', e)}\")\n",
        "        raise\n",
        "\n",
        "def gh_list_issues(gh: Github, repo_full_name: str, state: str = \"open\", limit: int = 5):\n",
        "    print(f\"Listing last {limit} '{state}' issues for {repo_full_name}...\")\n",
        "    repo = gh_get_repo(gh, repo_full_name)\n",
        "\n",
        "    issues_pl = repo.get_issues(state=state)\n",
        "    issues_pl = cast(PaginatedList, issues_pl)  # PaginatedList[Issue] at runtime\n",
        "\n",
        "    out: List[dict] = []\n",
        "    # safer than slicing a PaginatedList for some stub checkers\n",
        "    for i, issue in enumerate(issues_pl):\n",
        "        if i >= limit:\n",
        "            break\n",
        "        issue = cast(Issue, issue)\n",
        "        out.append({\n",
        "            \"number\": issue.number,\n",
        "            \"title\": issue.title,\n",
        "            \"state\": issue.state,\n",
        "            \"url\": issue.html_url,\n",
        "        })\n",
        "    print(f\"Found {len(out)} issues.\")\n",
        "    return out\n",
        "\n",
        "def gh_create_issue(gh: Github, repo_full_name: str, title: str, body: str = \"\", labels: Optional[List[str]] = None) -> int:\n",
        "    print(f\"Creating issue in {repo_full_name}: '{title}'\")\n",
        "    repo = gh_get_repo(gh, repo_full_name)\n",
        "    issue = repo.create_issue(title=title, body=body, labels=labels or [])\n",
        "    issue = cast(Issue, issue)\n",
        "    print(f\"Issue #{issue.number} created successfully.\")\n",
        "    return issue.number\n",
        "\n",
        "def gh_comment_issue(gh: Github, repo_full_name: str, number: int, body: str) -> str:\n",
        "    print(f\"Commenting on issue #{number} in {repo_full_name}...\")\n",
        "    repo = gh_get_repo(gh, repo_full_name)\n",
        "    issue = cast(Issue, repo.get_issue(number=number))\n",
        "    comment = issue.create_comment(body)\n",
        "    # comment is a GithubObject; just use getattr to keep pyright happy\n",
        "    url = getattr(comment, \"html_url\", \"\")\n",
        "    print(f\"Comment posted: {url}\")\n",
        "    return url"
      ],
      "id": "fBKSXkU-71ek"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUlxHv6t71ek"
      },
      "source": [
        "## 8. Google Drive & GitHub File Ops (REST API)\n",
        "\n",
        "Functions for saving generated files to Google Drive and the GitHub repository."
      ],
      "id": "bUlxHv6t71ek"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "P5aiRU9Z71ek"
      },
      "outputs": [],
      "source": [
        "# ========== Google Drive (mounted) ==========\n",
        "def ensure_drive_ready() -> bool:\n",
        "    \"\"\"Mounts Drive (if in Colab) and ensures the output folder exists.\"\"\"\n",
        "    if not COLAB_ENV:\n",
        "        print(\"[drive] Not in Colab; skipping mount.\")\n",
        "        return False\n",
        "    try:\n",
        "        print(\"[drive] Mounting…\")\n",
        "        drive.mount(GDRIVE_MOUNT_POINT)\n",
        "        os.makedirs(OUTPUT_FOLDER_PATH, exist_ok=True)\n",
        "        print(f\"[drive] Ready: {OUTPUT_FOLDER_PATH}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"[drive] ERROR mounting Drive: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "def save_to_drive(rel_path: str, content: str | bytes, binary: bool = False) -> str:\n",
        "    \"\"\"\n",
        "    Saves a file under your Drive output folder.\n",
        "    rel_path: path relative to OUTPUT_FOLDER_PATH (e.g., 'plans/wrangler.toml').\n",
        "    content:  str or bytes. Set binary=True if bytes.\n",
        "    \"\"\"\n",
        "    if not COLAB_ENV:\n",
        "        print(\"[drive] Google Drive save requires Colab (mounted Drive). Skipping.\")\n",
        "        return \"\"\n",
        "\n",
        "    abs_path = os.path.join(OUTPUT_FOLDER_PATH, rel_path)\n",
        "    os.makedirs(os.path.dirname(abs_path), exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        mode = \"wb\" if binary else \"w\"\n",
        "        encoding = None if binary else \"utf-8\"\n",
        "        with open(abs_path, mode, encoding=encoding) as f:\n",
        "            f.write(content)\n",
        "        print(f\"[drive] Saved → {abs_path}\")\n",
        "        return abs_path\n",
        "    except Exception as e:\n",
        "        print(f\"[drive] ERROR saving to Drive: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "# ========== GitHub (Contents API) ==========\n",
        "_GH_API = \"https://api.github.com\"\n",
        "\n",
        "def _gh_headers() -> dict:\n",
        "    if not GITHUB_TOKEN:\n",
        "        raise RuntimeError(\"GITHUB_TOKEN is missing (set in Colab secrets or env).\")\n",
        "    return {\n",
        "        \"Authorization\": f\"Bearer {GITHUB_TOKEN}\",\n",
        "        \"Accept\": \"application/vnd.github+json\",\n",
        "        \"X-GitHub-Api-Version\": \"2022-11-28\",\n",
        "        \"User-Agent\": \"genai-colab-uploader\"\n",
        "    }\n",
        "\n",
        "def _gh_get_default_branch(repo_full: str) -> str:\n",
        "    r = requests.get(f\"{_GH_API}/repos/{repo_full}\", headers=_gh_headers(), timeout=30)\n",
        "    r.raise_for_status()\n",
        "    return r.json().get(\"default_branch\", \"main\")\n",
        "\n",
        "def _gh_get_file_sha(repo_full: str, path: str, ref: Optional[str] = None) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Return SHA if file exists on branch/ref, else None.\n",
        "    \"\"\"\n",
        "    params = {\"ref\": ref} if ref else {}\n",
        "    r = requests.get(f\"{_GH_API}/repos/{repo_full}/contents/{path}\", headers=_gh_headers(), params=params, timeout=30)\n",
        "    if r.status_code == 404:\n",
        "        return None\n",
        "    r.raise_for_status()\n",
        "    return r.json().get(\"sha\")\n",
        "\n",
        "def _gh_put_contents(repo_full: str, path: str, message: str, b64content: str, branch: Optional[str], sha: Optional[str]):\n",
        "    payload = {\"message\": message, \"content\": b64content}\n",
        "    if branch:\n",
        "        payload[\"branch\"] = branch\n",
        "    if sha:\n",
        "        payload[\"sha\"] = sha\n",
        "    r = requests.put(f\"{_GH_API}/repos/{repo_full}/contents/{path}\", headers=_gh_headers(), json=payload, timeout=60)\n",
        "    # Helpful error surfacing\n",
        "    if r.status_code >= 400:\n",
        "        try:\n",
        "            print(\"[github] Error payload:\", r.json())\n",
        "        except Exception:\n",
        "            print(\"[github] Error text:\", r.text)\n",
        "    r.raise_for_status()\n",
        "    return r.json()\n",
        "\n",
        "def save_to_github(\n",
        "    path: str,\n",
        "    content: str | bytes,\n",
        "    commit_message: str,\n",
        "    repo_full: str = GITHUB_REPO,\n",
        "    branch: Optional[str] = None,\n",
        "    max_retries: int = 3\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Create or update a file in GitHub using the Contents API.\n",
        "    - path: repo-relative path (e.g., 'src/index.ts')\n",
        "    - content: str or bytes\n",
        "    - commit_message: commit title/message\n",
        "    - branch: target branch; if None, uses default branch\n",
        "    Returns the GitHub API response JSON (includes content download URL, commit SHA, etc.)\n",
        "    \"\"\"\n",
        "    if branch is None:\n",
        "        branch = _gh_get_default_branch(repo_full)\n",
        "\n",
        "    # Normalize to bytes + base64\n",
        "    content_bytes = content if isinstance(content, (bytes, bytearray)) else content.encode(\"utf-8\")\n",
        "    b64 = base64.b64encode(content_bytes).decode(\"ascii\")\n",
        "\n",
        "    # If file exists, we must send its current SHA\n",
        "    sha = _gh_get_file_sha(repo_full, path, ref=branch)\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            print(f\"[github] PUT {repo_full}:{branch}/{path} (attempt {attempt+1})\")\n",
        "            return _gh_put_contents(repo_full, path, commit_message, b64, branch, sha)\n",
        "        except requests.HTTPError as e:\n",
        "            code = getattr(e.response, \"status_code\", None)\n",
        "            if code in (429, 502, 503, 504) and attempt < max_retries - 1:\n",
        "                delay = min(2 ** attempt, 30) + 0.2 * (attempt + 1)\n",
        "                print(f\"[github] {code} retrying in {delay:.1f}s …\")\n",
        "                time.sleep(delay)\n",
        "                continue\n",
        "            raise\n",
        "    # This line is theoretically unreachable due to `raise` in the loop,\n",
        "    # but added for clarity that retries were exhausted.\n",
        "    raise RuntimeError(f\"Failed to upload to GitHub after {max_retries} attempts.\")"
      ],
      "id": "P5aiRU9Z71ek"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArNeT6hO71ek"
      },
      "source": [
        "## 9. Cost Report Generation\n",
        "\n",
        "Function to generate the final itemized cost report."
      ],
      "id": "ArNeT6hO71ek"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5gfo3mws71ek"
      },
      "outputs": [],
      "source": [
        "def generate_cost_report() -> str:\n",
        "    \"\"\"\n",
        "    Generates a formatted Markdown string from the COST_LEDGER.\n",
        "    \"\"\"\n",
        "    print(\"Generating cost report...\")\n",
        "    global COST_LEDGER, TOTAL_ESTIMATED_COST, COST_LIMIT_USD\n",
        "\n",
        "    report_lines = [\n",
        "        \"# Agent Run Cost Report\",\n",
        "        f\"**Run completed:** {time.ctime()}\",\n",
        "        f\"**Cost Limit:** ${COST_LIMIT_USD:.2f}\",\n",
        "        f\"**Total Estimated Cost:** ${TOTAL_ESTIMATED_COST:.6f}\",\n",
        "        \"\\n## Itemized Costs\\n\",\n",
        "        \"| Task | Details (Tokens) | Cost (USD) |\",\n",
        "        \"| :--- | :--- | :--- |\"\n",
        "    ]\n",
        "\n",
        "    for item in COST_LEDGER:\n",
        "        task = item.get(\"task\", \"Unknown Task\")\n",
        "        details = item.get(\"details\", \"N/A\")\n",
        "        cost = item.get(\"cost\", 0.0)\n",
        "        report_lines.append(f\"| {task} | {details} | ${cost:.6f} |\")\n",
        "\n",
        "    report_lines.append(\"\\n---\\n*End of Report*\")\n",
        "    return \"\\n\".join(report_lines)"
      ],
      "id": "5gfo3mws71ek"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB4_pUoZ71ek"
      },
      "source": [
        "## 10. Main Execution\n",
        "\n",
        "Run this cell to execute the full agent workflow."
      ],
      "id": "IB4_pUoZ71ek"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Irqj414Y71ek",
        "outputId": "6857ae27-8996-45dc-dea5-f9ea8cf2643a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Colby Agentic Crew - GenAI + GitHub Ops\n",
            "==================================================\n",
            "Cost Limit set to: $25.00\n",
            "GitHub Repo set to: jmbish04/colby-agentic-crew\n",
            "NOTE: Make sure 'GOOGLE_API_KEY' and 'GITHUB_TOKEN' are set in Colab secrets (View > Show secrets).\n",
            "[drive] Mounting…\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[drive] Ready: /content/drive/MyDrive/colby_agentic_crew\n",
            "Initializing clients...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1520374863.py:26: DeprecationWarning: Argument login_or_token is deprecated, please use auth=github.Auth.Token(...) instead\n",
            "  gh = Github(GITHUB_TOKEN, per_page=50)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GitHub client authenticated.\n",
            "All clients initialized successfully.\n",
            "\n",
            "--- Starting Task: Generate wrangler.toml ---\n",
            "Generating text for prompt: '\n",
            "        Generate a complete 'wrangler.toml' file ...'\n",
            "Warning: Could not pre-count tokens: system_instruction parameter is not supported in Gemini API.. Proceeding with rough estimate.\n",
            "[CostTracker] Task 'Generate wrangler.toml' cost: $0.000632\n",
            "[CostTracker] New total estimated cost: $0.0006\n",
            "Text generation successful.\n",
            "Generated wrangler.toml:\n",
            " As an expert AI assistant specializing in Cloudflare Workers and agentic systems, here is the complete, production-ready `wrangler.toml` configuration for your project, `colby-agentic-crew`.\n",
            "\n",
            "This configuration includes all specified bindings, using placeholders for IDs that must be generated when you run `wrangler deploy` or `wrangler d1 create`/`wrangler kv namespace create`.\n",
            "\n",
            "## `wrangler.toml`\n",
            "\n",
            "```toml\n",
            "# ==============================================================================\n",
            "# CORE WORKER CONFIGURATION\n",
            "# ==============================================================================\n",
            "name = \"colby-agentic-crew\"\n",
            "main = \"src/index.ts\"\n",
            "compatibility_date = \"2024-05-15\"\n",
            "workers_dev = true\n",
            "\n",
            "# Assuming you are using TypeScript, ensure your build process handles it.\n",
            "# If using the standard 'npm create cloudflare@latest', this is usually handled.\n",
            "# build = { command = \"npm run build\" }\n",
            "\n",
            "\n",
            "# ==============================================================================\n",
            "# DURABLE OBJECTS (AGENT STATE MANAGEMENT)\n",
            "# Binding: AGENT_SUPERVISOR\n",
            "# Class: AgentSupervisor\n",
            "# ==============================================================================\n",
            "[[durable_objects.bindings]]\n",
            "name = \"AGENT_SUPERVISOR\"\n",
            "class_name = \"AgentSupervisor\"\n",
            "\n",
            "# Define the Durable Object migration configuration\n",
            "# This ensures the DO namespace is created and managed.\n",
            "[[migrations]]\n",
            "tag = \"v1\"\n",
            "new_classes = [\"AgentSupervisor\"]\n",
            "\n",
            "\n",
            "# ==============================================================================\n",
            "# KV NAMESPACE (CREW MEMORY & CONFIGURATION)\n",
            "# Binding: CREW_MEMORY_KV\n",
            "# ==============================================================================\n",
            "[[kv_namespaces]]\n",
            "binding = \"CREW_MEMORY_KV\"\n",
            "# IMPORTANT: Replace this placeholder ID after running:\n",
            "# wrangler kv namespace create CREW_MEMORY_KV\n",
            "id = \"CREW_MEMORY_KV_ID_PLACEHOLDER\"\n",
            "\n",
            "\n",
            "# ==============================================================================\n",
            "# D1 DATABASE (CREW STATE & LOGGING)\n",
            "# Binding: CREW_STATE_DB\n",
            "# ==============================================================================\n",
            "[[d1_databases]]\n",
            "binding = \"CREW_STATE_DB\"\n",
            "database_name = \"colby_crew_state\"\n",
            "# IMPORTANT: Replace this placeholder ID after running:\n",
            "# wrangler d1 create colby_crew_state\n",
            "database_id = \"CREW_STATE_DB_ID_PLACEHOLDER\"\n",
            "\n",
            "\n",
            "# ==============================================================================\n",
            "# R2 BUCKET (AGENT ARTIFACTS & LARGE FILE STORAGE)\n",
            "# Binding: AGENT_ARTIFACTS_R2\n",
            "# ==============================================================================\n",
            "[[r2_buckets]]\n",
            "binding = \"AGENT_ARTIFACTS_R2\"\n",
            "bucket_name = \"colby-agent-artifacts\"\n",
            "# Note: R2 buckets are globally unique by name, so no ID is required here.\n",
            "\n",
            "# ==============================================================================\n",
            "# ENVIRONMENT VARIABLES (Example: API Keys)\n",
            "# ==============================================================================\n",
            "[vars]\n",
            "# OPENAI_API_KEY = \"...\"\n",
            "# AGENT_LOG_LEVEL = \"info\"\n",
            "```\n",
            "\n",
            "### Next Steps (Required Setup)\n",
            "\n",
            "Before deploying, you must create the resources bound by ID:\n",
            "\n",
            "1.  **Create KV Namespace:**\n",
            "    ```bash\n",
            "    wrangler kv namespace create CREW_MEMORY_KV\n",
            "    # Copy the resulting ID into the `wrangler.toml` file.\n",
            "    ```\n",
            "\n",
            "2.  **Create D1 Database:**\n",
            "    ```bash\n",
            "    wrangler d1 create colby_crew_state\n",
            "    # Copy the resulting database_id into the `wrangler.toml` file.\n",
            "    ```\n",
            "\n",
            "3.  **Deploy the Worker:**\n",
            "    ```bash\n",
            "    wrangler deploy\n",
            "    ```\n",
            "\n",
            "--- Saving wrangler.toml to Google Drive ---\n",
            "[drive] Saved → /content/drive/MyDrive/colby_agentic_crew/wrangler_config_1761422638.toml\n",
            "\n",
            "--- Saving wrangler.toml to GitHub ---\n",
            "[github] PUT jmbish04/colby-agentic-crew:main/agent_generated_files/wrangler_config_1761422638.toml (attempt 1)\n",
            "\n",
            "--- Starting Task: Generate README.md ---\n",
            "Generating text for prompt: '\n",
            "        Generate a professional README.md for the...'\n",
            "Warning: Could not pre-count tokens: system_instruction parameter is not supported in Gemini API.. Proceeding with rough estimate.\n",
            "[CostTracker] Task 'Generate README.md' cost: $0.000907\n",
            "[CostTracker] New total estimated cost: $0.0015\n",
            "Text generation successful.\n",
            "Generated README.md:\n",
            " The following is a professional, production-ready `README.md` for your project, detailing the architecture and configuration using the specified Cloudflare services and agentic system concepts.\n",
            "\n",
            "---\n",
            "\n",
            "# `colby-agentic-crew`\n",
            "\n",
            "**Serverless Agentic Orchestration powered by Cloudflare and CrewAI**\n",
            "\n",
            "[![Cloudflare Workers](https://img.shields.io/badge/Cloudflare-Workers-orange?style=flat-square)](https://workers.cloudflare.com/)\n",
            "[![Durable Objects](https://img.shields.io/badge/State-Durable%20Objects-blueviolet?style=flat-square)](https://developers.cloudflare.com/durable-objects/)\n",
            "[![Database](https://img.shields.io/badge/Database-D1-lightgray?style=flat-square)](https://developers.cloudflare.com/d1/)\n",
            "[![Storage](https://img.shields.io/badge/Storage-R2-teal?style=flat-square)](https://developers.cloudflare.com/r2/)\n",
            "[![Configuration](https://img.shields.io/badge/Config-KV-yellow?style=flat-square)](https://developers.cloudflare.com/kv/)\n",
            "\n",
            "## 📝 Description\n",
            "\n",
            "The `colby-agentic-crew` project provides a highly scalable, stateful, and serverless infrastructure for running complex agentic workflows orchestrated by frameworks like CrewAI.\n",
            "\n",
            "By leveraging the Cloudflare ecosystem, this system ensures low-latency execution, persistent state management via Durable Objects, structured logging via D1, and robust artifact storage via R2, making it ideal for long-running, resource-intensive AI tasks.\n",
            "\n",
            "## ✨ Features\n",
            "\n",
            "*   **Stateful Execution:** Durable Objects (DOs) manage the lifecycle and state of individual agent crews, ensuring persistence and fault tolerance throughout multi-step processes.\n",
            "*   **Scalability:** Cloudflare Workers handle API requests, automatically scaling to meet demand without requiring traditional server management.\n",
            "*   **Integrated Data Storage:**\n",
            "    *   **D1:** Used for structured data, logging crew history, agent performance metrics, and task definitions.\n",
            "    *   **R2:** Used for storing large outputs, generated reports, images, or datasets produced by the agent crews.\n",
            "*   **Dynamic Configuration:** Cloudflare KV provides fast, global access to configuration settings, API keys, and feature flags.\n",
            "*   **Unified Configuration:** All service bindings and environment variables are centrally managed within the `wrangler.toml` file.\n",
            "\n",
            "## 🏗️ Architecture Overview\n",
            "\n",
            "| Component | Cloudflare Service | Role in Agentic System |\n",
            "| :--- | :--- | :--- |\n",
            "| **API Gateway** | Cloudflare Worker | Receives requests (e.g., `POST /start_crew`), handles routing, authentication, and initial validation. |\n",
            "| **Crew Manager** | Durable Object (DO) | The stateful core. Each DO instance represents a single running CrewAI process, managing its internal state, communication, and task execution lifecycle. |\n",
            "| **Structured Data** | D1 Database | Stores metadata about crews, detailed execution logs, agent definitions, and user data. |\n",
            "| **Artifact Storage** | R2 Bucket | Stores final outputs, large files, and intermediate results generated by the agents. |\n",
            "| **Global Config** | KV Namespace | Stores runtime configuration, LLM API keys, system prompts, and feature toggles accessible by Workers and DOs. |\n",
            "\n",
            "## ⚙️ Prerequisites\n",
            "\n",
            "To deploy and run this project, you will need:\n",
            "\n",
            "1.  **Cloudflare Account:** Access to the Cloudflare dashboard and API tokens.\n",
            "2.  **Node.js & npm:** (LTS recommended) for running `wrangler`.\n",
            "3.  **`wrangler` CLI:** The Cloudflare command-line tool.\n",
            "    ```bash\n",
            "    npm install -g wrangler\n",
            "    ```\n",
            "4.  **Python Environment:** (If integrating the actual CrewAI logic) While the Worker/DO environment is JavaScript/TypeScript, the agent logic often runs in a subprocess or via an external service triggered by the DO. For local development or testing the agent logic, a Python environment is necessary.\n",
            "\n",
            "## 🚀 Setup and Installation\n",
            "\n",
            "### 1. Clone the Repository\n",
            "\n",
            "```bash\n",
            "git clone https://github.com/your-org/colby-agentic-crew.git\n",
            "cd colby-agentic-crew\n",
            "```\n",
            "\n",
            "### 2. Install Dependencies\n",
            "\n",
            "```bash\n",
            "npm install\n",
            "```\n",
            "\n",
            "### 3. Authenticate Wrangler\n",
            "\n",
            "Log in to your Cloudflare account via the CLI:\n",
            "\n",
            "```bash\n",
            "wrangler login\n",
            "```\n",
            "\n",
            "### 4. Provision Resources\n",
            "\n",
            "Before deployment, you must create the necessary resources (D1 database, R2 bucket, and KV namespace) via the `wrangler` CLI or the Cloudflare dashboard.\n",
            "\n",
            "**Example Provisioning Commands:**\n",
            "\n",
            "```bash\n",
            "# Create D1 Database\n",
            "wrangler d1 create colby-crew-db\n",
            "\n",
            "# Create R2 Bucket\n",
            "wrangler r2 bucket create colby-crew-artifacts\n",
            "\n",
            "# Create KV Namespace\n",
            "wrangler kv namespace create colby_crew_config\n",
            "```\n",
            "\n",
            "## 🛠️ Configuration (`wrangler.toml`)\n",
            "\n",
            "The `wrangler.toml` file is the central configuration hub, defining the Worker entry point, environment variables, and crucially, the bindings to all necessary Cloudflare services.\n",
            "\n",
            "Ensure your `wrangler.toml` is correctly configured with the IDs and names generated during the provisioning step.\n",
            "\n",
            "### Example `wrangler.toml` Structure\n",
            "\n",
            "```toml\n",
            "name = \"colby-agentic-crew\"\n",
            "main = \"src/index.ts\"\n",
            "compatibility_date = \"2024-01-01\"\n",
            "workers_dev = true\n",
            "\n",
            "[vars]\n",
            "# Example environment variable accessible in the Worker/DO\n",
            "LLM_MODEL = \"gpt-4o\"\n",
            "\n",
            "# ---\n",
            "\n",
            "--- Saving README.md to Google Drive ---\n",
            "[drive] Saved → /content/drive/MyDrive/colby_agentic_crew/README_1761422638.md\n",
            "\n",
            "--- Saving README.md to GitHub ---\n",
            "[github] PUT jmbish04/colby-agentic-crew:main/agent_generated_files/README_1761422638.md (attempt 1)\n",
            "\n",
            "--- 6. Performing GitHub Issue Operations ---\n",
            "Listing last 5 'open' issues for jmbish04/colby-agentic-crew...\n",
            "Found 0 issues.\n",
            "Existing open issues: []\n",
            "Creating issue in jmbish04/colby-agentic-crew: 'Bot Task: Review new `wrangler.toml` (1761422638)'\n",
            "Issue #1 created successfully.\n",
            "Commenting on issue #1 in jmbish04/colby-agentic-crew...\n",
            "Comment posted: https://github.com/jmbish04/colby-agentic-crew/issues/1#issuecomment-3447736045\n",
            "\n",
            "==================================================\n",
            "--- 7. Generating Final Cost Report ---\n",
            "Generating cost report...\n",
            "# Agent Run Cost Report\n",
            "**Run completed:** Sat Oct 25 20:04:27 2025\n",
            "**Cost Limit:** $25.00\n",
            "**Total Estimated Cost:** $0.001539\n",
            "\n",
            "## Itemized Costs\n",
            "\n",
            "| Task | Details (Tokens) | Cost (USD) |\n",
            "| :--- | :--- | :--- |\n",
            "| Main Execution (CRITICAL FAIL) | Models.generate_content() got an unexpected keyword argument 'system_instruction' | $0.000000 |\n",
            "| Main Execution (CRITICAL FAIL) | Models.generate_content() got an unexpected keyword argument 'system_instruction' | $0.000000 |\n",
            "| Generate wrangler.toml | 165 in, 820 out | $0.000632 |\n",
            "| Save wrangler.toml to Drive | File I/O | $0.000000 |\n",
            "| Save wrangler.toml to GitHub | GitHub API call | $0.000000 |\n",
            "| Generate README.md | 101 in, 1245 out | $0.000907 |\n",
            "| Save README.md to Drive | File I/O | $0.000000 |\n",
            "| Save README.md to GitHub | GitHub API call | $0.000000 |\n",
            "| List GitHub Issues | GitHub API call | $0.000000 |\n",
            "| Create GitHub Issue (wrangler.toml) | Created #1 | $0.000000 |\n",
            "| Comment on GitHub Issue | Commented on #1 | $0.000000 |\n",
            "\n",
            "---\n",
            "*End of Report*\n",
            "\n",
            "--- 8. Saving Final Cost Report ---\n",
            "[drive] Saved → /content/drive/MyDrive/colby_agentic_crew/cost_report_1761422638.md\n",
            "[github] PUT jmbish04/colby-agentic-crew:main/agent_generated_files/cost_report_1761422638.md (attempt 1)\n",
            "\n",
            "==================================================\n",
            "Script finished.\n",
            "FINAL ESTIMATED COST: $0.001539\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*50)\n",
        "print(\"Colby Agentic Crew - GenAI + GitHub Ops\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Cost Limit set to: ${COST_LIMIT_USD:.2f}\")\n",
        "print(f\"GitHub Repo set to: {GITHUB_REPO}\")\n",
        "print(\"NOTE: Make sure 'GOOGLE_API_KEY' and 'GITHUB_TOKEN' are set in Colab secrets (View > Show secrets).\")\n",
        "\n",
        "# Mount drive first\n",
        "drive_ready = ensure_drive_ready()\n",
        "\n",
        "generated_text_toml = None\n",
        "generated_text_readme = None\n",
        "timestamp = int(time.time())\n",
        "\n",
        "try:\n",
        "    if not drive_ready and COLAB_ENV:\n",
        "        print(\"Could not mount Google Drive. Aborting to prevent lost work.\")\n",
        "        sys.exit(1) # Use sys.exit to trigger finally\n",
        "\n",
        "    # Initialize clients\n",
        "    clients = init_clients()\n",
        "\n",
        "    if clients:\n",
        "        # --- 1. Generate Text Artifact (wrangler.toml) ---\n",
        "        text_prompt_toml = \"\"\"\n",
        "        Generate a complete 'wrangler.toml' file for a Cloudflare agentic crew project named 'colby-agentic-crew'.\n",
        "        The project must include:\n",
        "        1. A main worker entrypoint 'src/index.ts'.\n",
        "        2. A Durable Object binding named 'AGENT_SUPERVISOR' for the class 'AgentSupervisor'.\n",
        "        3. A KV namespace binding named 'CREW_MEMORY_KV'.\n",
        "        4. A D1 database binding named 'CREW_STATE_DB'.\n",
        "        5. An R2 bucket binding named 'AGENT_ARTIFACTS_R2'.\n",
        "        \"\"\"\n",
        "        generated_text_toml = gen_text(\n",
        "            clients,\n",
        "            task_name=\"Generate wrangler.toml\",\n",
        "            prompt=text_prompt_toml\n",
        "        )\n",
        "\n",
        "        if generated_text_toml:\n",
        "            print(\"Generated wrangler.toml:\\n\", generated_text_toml)\n",
        "\n",
        "            # --- 2. Save Text to Drive ---\n",
        "            print(\"\\n--- Saving wrangler.toml to Google Drive ---\")\n",
        "            output_filename_toml = f\"wrangler_config_{timestamp}.toml\"\n",
        "            save_to_drive(output_filename_toml, generated_text_toml)\n",
        "            COST_LEDGER.append({\"task\": \"Save wrangler.toml to Drive\", \"cost\": 0.0, \"details\": \"File I/O\"})\n",
        "\n",
        "            # --- 3. Save Text to GitHub ---\n",
        "            print(\"\\n--- Saving wrangler.toml to GitHub ---\")\n",
        "            try:\n",
        "                gh_path_toml = f\"{GITHUB_SAVE_PATH}/{output_filename_toml}\"\n",
        "                gh_message_toml = f\"Agent: Add wrangler.toml config ({timestamp})\"\n",
        "                save_to_github(\n",
        "                    path=gh_path_toml,\n",
        "                    content=generated_text_toml,\n",
        "                    commit_message=gh_message_toml\n",
        "                )\n",
        "                COST_LEDGER.append({\"task\": \"Save wrangler.toml to GitHub\", \"cost\": 0.0, \"details\": \"GitHub API call\"})\n",
        "            except Exception as e:\n",
        "                print(f\"Error saving text to GitHub: {e}\")\n",
        "                COST_LEDGER.append({\"task\": \"Save wrangler.toml to GitHub (FAILED)\", \"cost\": 0.0, \"details\": str(e)})\n",
        "        else:\n",
        "            print(\"Skipping wrangler.toml saving steps as generation failed or was cost-blocked.\")\n",
        "\n",
        "        # --- 4. Generate Text Artifact (README.md) ---\n",
        "        text_prompt_readme = \"\"\"\n",
        "        Generate a professional README.md for the 'colby-agentic-crew' project.\n",
        "        It should describe a system using Cloudflare Workers, Durable Objects, D1, R2, and KV for running agentic crews.\n",
        "        Mention that the configuration is managed in 'wrangler.toml'.\n",
        "        \"\"\"\n",
        "        generated_text_readme = gen_text(\n",
        "            clients,\n",
        "            task_name=\"Generate README.md\",\n",
        "            prompt=text_prompt_readme\n",
        "        )\n",
        "\n",
        "        if generated_text_readme:\n",
        "            print(\"Generated README.md:\\n\", generated_text_readme)\n",
        "\n",
        "            # --- 5. Save README to Drive & GitHub ---\n",
        "            print(\"\\n--- Saving README.md to Google Drive ---\")\n",
        "            output_filename_readme = f\"README_{timestamp}.md\"\n",
        "            save_to_drive(output_filename_readme, generated_text_readme)\n",
        "            COST_LEDGER.append({\"task\": \"Save README.md to Drive\", \"cost\": 0.0, \"details\": \"File I/O\"})\n",
        "\n",
        "            print(\"\\n--- Saving README.md to GitHub ---\")\n",
        "            try:\n",
        "                gh_path_readme = f\"{GITHUB_SAVE_PATH}/{output_filename_readme}\"\n",
        "                gh_message_readme = f\"Agent: Add README.md documentation ({timestamp})\"\n",
        "                save_to_github(\n",
        "                    path=gh_path_readme,\n",
        "                    content=generated_text_readme,\n",
        "                    commit_message=gh_message_readme\n",
        "                )\n",
        "                COST_LEDGER.append({\"task\": \"Save README.md to GitHub\", \"cost\": 0.0, \"details\": \"GitHub API call\"})\n",
        "            except Exception as e:\n",
        "                print(f\"Error saving README to GitHub: {e}\")\n",
        "                COST_LEDGER.append({\"task\": \"Save README.md to GitHub (FAILED)\", \"cost\": 0.0, \"details\": str(e)})\n",
        "        else:\n",
        "             print(\"Skipping README.md saving steps as generation failed or was cost-blocked.\")\n",
        "\n",
        "        # --- 6. Perform GitHub Issue Operations ---\n",
        "        print(\"\\n--- 6. Performing GitHub Issue Operations ---\")\n",
        "        if GITHUB_REPO == \"jmbish04/colby-agentic-crew\": # Check if it's the real repo\n",
        "            try:\n",
        "                # List existing issues\n",
        "                issues = gh_list_issues(clients.gh, GITHUB_REPO, state=\"open\", limit=5)\n",
        "                print(\"Existing open issues:\", json.dumps(issues, indent=2))\n",
        "                COST_LEDGER.append({\"task\": \"List GitHub Issues\", \"cost\": 0.0, \"details\": \"GitHub API call\"})\n",
        "\n",
        "                if generated_text_toml:\n",
        "                    # Create a new issue only if text was generated\n",
        "                    issue_title = f\"Bot Task: Review new `wrangler.toml` ({timestamp})\"\n",
        "                    issue_body = f\"Generated by agent.\\n\\nSee file in repo: `{gh_path_toml}`\\n\\n**Configuration:**\\n```toml\\n{generated_text_toml}\\n```\"\n",
        "                    new_issue_num = gh_create_issue(clients.gh, GITHUB_REPO, issue_title, issue_body, labels=[\"bot\", \"config\"])\n",
        "                    COST_LEDGER.append({\"task\": \"Create GitHub Issue (wrangler.toml)\", \"cost\": 0.0, \"details\": f\"Created #{new_issue_num}\"})\n",
        "\n",
        "                    # Comment on the new issue\n",
        "                    comment_body = f\"Task #{new_issue_num} created.\"\n",
        "                    if generated_text_readme:\n",
        "                        comment_body += f\"\\nAssociated README also generated: `{gh_path_readme}`\"\n",
        "\n",
        "                    gh_comment_issue(clients.gh, GITHUB_REPO, new_issue_num, comment_body)\n",
        "                    COST_LEDGER.append({\"task\": \"Comment on GitHub Issue\", \"cost\": 0.0, \"details\": f\"Commented on #{new_issue_num}\"})\n",
        "                else:\n",
        "                    print(\"Skipping issue creation as text generation failed or was cost-blocked.\")\n",
        "\n",
        "            except GithubException as e:\n",
        "                print(f\"Error during GitHub operations: {e}\")\n",
        "                print(f\"Please ensure the token has 'repo' scope and access to '{GITHUB_REPO}'.\")\n",
        "                COST_LEDGER.append({\"task\": \"GitHub Issue Ops (FAILED)\", \"cost\": 0.0, \"details\": str(e)})\n",
        "            except Exception as e:\n",
        "                print(f\"An unexpected error occurred during GitHub ops: {e}\")\n",
        "                COST_LEDGER.append({\"task\": \"GitHub Issue Ops (FAILED)\", \"cost\": 0.0, \"details\": str(e)})\n",
        "    else:\n",
        "        print(\"Failed to initialize clients. Check secrets.\")\n",
        "        COST_LEDGER.append({\"task\": \"Client Initialization\", \"cost\": 0.0, \"details\": \"FAILED\"})\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An uncaught error occurred in main execution: {e}\")\n",
        "    COST_LEDGER.append({\"task\": \"Main Execution (CRITICAL FAIL)\", \"cost\": 0.0, \"details\": str(e)})\n",
        "\n",
        "finally:\n",
        "    # --- 7. Generate and Save Final Cost Report ---\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"--- 7. Generating Final Cost Report ---\")\n",
        "    report_content = generate_cost_report()\n",
        "    print(report_content)\n",
        "\n",
        "    print(\"\\n--- 8. Saving Final Cost Report ---\")\n",
        "    report_filename = f\"cost_report_{timestamp}.md\"\n",
        "\n",
        "    # Save to Drive\n",
        "    save_to_drive(report_filename, report_content)\n",
        "\n",
        "    # Save to GitHub\n",
        "    try:\n",
        "        save_to_github(\n",
        "            path=f\"{GITHUB_SAVE_PATH}/{report_filename}\",\n",
        "            content=report_content,\n",
        "            commit_message=f\"Agent: Final cost report ({timestamp})\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving final cost report to GitHub: {e}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Script finished.\")\n",
        "    print(f\"FINAL ESTIMATED COST: ${TOTAL_ESTIMATED_COST:.6f}\")\n",
        "    print(\"=\"*50)"
      ],
      "id": "Irqj414Y71ek"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
